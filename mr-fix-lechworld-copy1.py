#!/usr/bin/env python3
"""
Mr. Fix-My-Project-Please - Universal Project Intelligence Analyzer
The ONE GLOBAL SCRIPT that fixes any project, any scale, any complexity
Maximum information extraction with LLM-powered insights and rich HTML reporting
Adaptive 3-minute analysis with Plan B fallbacks for every scenario
"""

import os
import json
import hashlib
import datetime
import math
import time
import psutil
import threading
import subprocess
from pathlib import Path
from collections import defaultdict, Counter
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from difflib import SequenceMatcher
import logging

# Setup logging
logging.basicConfig(level=logging.ERROR, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

class MrFixMyProjectPlease:
    def __init__(self, project_path: str = "."):
        self.project_path = Path(project_path).resolve()
        self.current_lang = 'en'
        self.translations = self.get_translations()

        # Performance targets
        self.MAX_ANALYSIS_TIME = 180  # 3 minutes
        self.MAX_MEMORY_GB = 2.0
        self.MAX_FILES_FOR_DEEP_ANALYSIS = 2000
        self.SAMPLING_RATE_LARGE = 0.05  # 5% for large projects
        self.SAMPLING_RATE_MEGA = 0.01   # 1% for mega ecosystems

        # Analysis state
        self.start_time = None
        self.abort_analysis = False
        self.memory_monitor_thread = None

        # Results storage
        self.surface_scan = {}
        self.deep_analysis = {}
        self.ecosystem_intelligence = {}
        self.performance_metrics = {}

        # NEW: Advanced analysis results
        self.temporal_analysis = {}
        self.duplicate_analysis = {}
        self.naming_analysis = {}
        self.directory_purposes = {}
        self.consolidation_opportunities = []
        self.tech_stack = {}
        self.empty_directories = []
        self.work_sessions = []
        self.monthly_activity = {}
        self.llm_insights = {}

        # Backwards compatibility
        self.problems = []
        self.opportunities = []
        self.naming_conventions = {}
        self.semantic_duplicates = []
        self.directory_analysis = {}
        self.files_data = []
        self.meta_purpose = None

        # Critical scoring weights
        self.scoring_weights = {
            'empty_directories': -25,
            'duplicate_files': -30,
            'naming_inconsistency': -20,
            'scattered_purposes': -25,
            'backup_mess': -15,
            'temp_files': -10,
            'root_clutter': -40,
            'good_structure': 15,
            'consistent_naming': 10,
            'good_organization': 20,
            'session_diversity': 5,
            'ecosystem_maturity': 25,
            'scale_efficiency': 10,
        }

    def get_translations(self):
        """Get translations for supported languages"""
        return {
            'en': {
                'title': 'ðŸ”¨ MAXIMUM INFORMATION EXTRACTION REPORT',
                'subtitle': '3-Minute Ecosystem Intelligence â€” Adaptive Analysis with Plan B Fallbacks',
                'project_health': 'Project Health',
                'health_score': 'Health Score',
                'analysis_time': 'Analysis Time',
                'efficiency_rating': 'Efficiency Rating',
                'files_per_second': 'Files/Second',
                'memory_usage': 'Memory Usage',
                'strategy_used': 'Analysis Strategy',
                'data_points_extracted': 'Data Points Extracted',
                'risk_factors_detected': 'Risk Factors Detected',
                'adaptability_score': 'Adaptability Score',
                'ecosystem_intelligence': 'Ecosystem Intelligence',
                'performance_metrics': 'Performance Metrics',
                'strategic_recommendations': 'Strategic Recommendations',
                'immediate_actions': 'Immediate Actions',
                'scale_assessment': 'Scale Assessment',
                'project_types': 'Project Types',
                'complexity_distribution': 'Complexity Distribution',
                'health_distribution': 'Health Distribution',
                'efficiency_metrics': 'Efficiency Metrics',
                'risk_analysis': 'Risk Analysis',
                'optimization_opportunities': 'Optimization Opportunities',
                'ecosystem_health': 'Ecosystem Health',
                'analysis_strategy': 'Analysis Strategy',
                'data_extraction_quality': 'Data Extraction Quality',
                'insights_density': 'Insights Density',
                'processing_efficiency': 'Processing Efficiency',
                'memory_efficiency': 'Memory Efficiency',
                'time_efficiency': 'Time Efficiency',
                'quality_score': 'Quality Score',
                'total_files': 'Total Files',
                'total_directories': 'Total Directories',
                'total_projects': 'Total Projects',
                'binary_files': 'Binary Files',
                'text_files': 'Text Files',
                'broken_symlinks': 'Broken Symlinks',
                'inaccessible_files': 'Inaccessible Files',
                'large_files': 'Large Files',
                'duplicate_files': 'Duplicate Files',
                'empty_directories': 'Empty Directories',
                'depth_analysis': 'Depth Analysis',
                'naming_conventions': 'Naming Conventions',
                'project_maturity': 'Project Maturity',
                'ecosystem_coordination': 'Ecosystem Coordination',
                'patterns_detected': 'Patterns Detected',
                'anomalies_detected': 'Anomalies Detected',
                'recommendations': 'Recommendations',
                'next_steps': 'Next Steps',
                'critical_issues': 'Critical Issues',
                'high_priority': 'High Priority',
                'medium_priority': 'Medium Priority',
                'low_priority': 'Low Priority',
                'improvements_needed': 'Improvements Needed',
                'strengths': 'Strengths',
                'weaknesses': 'Weaknesses',
                'opportunities': 'Opportunities',
                'threats': 'Threats',
                'score': 'Score',
                'grade': 'Grade',
                'status': 'Status',
                'excellent': 'Excellent',
                'good': 'Good',
                'needs_attention': 'Needs Attention',
                'critical': 'Critical',
                'mega_ecosystem': 'Mega Ecosystem',
                'large_ecosystem': 'Large Ecosystem',
                'medium_ecosystem': 'Medium Ecosystem',
                'small_ecosystem': 'Small Ecosystem',
                'single_project': 'Single Project',
                'web': 'Web',
                'python': 'Python',
                'rust': 'Rust',
                'javascript': 'JavaScript',
                'typescript': 'TypeScript',
                'go': 'Go',
                'java': 'Java',
                'cpp': 'C++',
                'other': 'Other',
                'high': 'High',
                'medium': 'Medium',
                'low': 'Low',
                'critical_severity': 'Critical',
                'high_severity': 'High',
                'medium_severity': 'Medium',
                'low_severity': 'Low',
                'immediate': 'Immediate',
                'weeks': 'weeks',
                'days': 'days',
                'hours': 'hours',
                'minutes': 'minutes',
                'seconds': 'seconds',
                'kb': 'KB',
                'mb': 'MB',
                'gb': 'GB',
                'tb': 'TB',
                'files': 'files',
                'directories': 'directories',
                'projects': 'projects',
                'levels': 'levels',
                'groups': 'groups',
                'guardrails': 'Guardrails',
                'project': 'Project',
                'analyzed': 'Analyzed',
                'penalty_weight': 'Penalty Weight',
                'optimal_state': 'Optimal State',
                'current_gap': 'Current Gap',
                'immediate_actions_required': 'Immediate Actions Required',
                'strategic_recommendations': 'Strategic Recommendations',
                'conclusion': 'Conclusion',
                'analysis_completed': 'Analysis Completed',
                'next_review': 'Next Review',
                'after_restructuring': 'After Restructuring',
                'critical_issues_summary': 'Critical Issues Summary',
                'project_grade': 'Project Grade',
                'recommendation': 'Recommendation',
                'allocate_minimum': 'Allocate minimum',
                'complete_project_restructuring': 'Complete project restructuring',
                'before_proceeding': 'Before proceeding with any new development',
                'immediate_attention_and_major_restructuring': 'Immediate attention and major restructuring',
                'requires_immediate_attention': 'Requires immediate attention',
                'restructuring': 'Restructuring',
            },
            'pt': {
                'title': 'ðŸ”¨ RELATÃ“RIO DE EXTRAÃ‡ÃƒO MÃXIMA DE INFORMAÃ‡Ã•ES',
                'subtitle': 'InteligÃªncia de Ecossistema em 3 Minutos â€” AnÃ¡lise Adaptativa com Planos B de ContingÃªncia',
                'project_health': 'SaÃºde do Projeto',
                'health_score': 'PontuaÃ§Ã£o de SaÃºde',
                'analysis_time': 'Tempo de AnÃ¡lise',
                'efficiency_rating': 'ClassificaÃ§Ã£o de EficiÃªncia',
                'files_per_second': 'Arquivos/Segundo',
                'memory_usage': 'Uso de MemÃ³ria',
                'strategy_used': 'EstratÃ©gia de AnÃ¡lise',
                'data_points_extracted': 'Pontos de Dados ExtraÃ­dos',
                'risk_factors_detected': 'Fatores de Risco Detectados',
                'adaptability_score': 'PontuaÃ§Ã£o de Adaptabilidade',
                'ecosystem_intelligence': 'InteligÃªncia do Ecossistema',
                'performance_metrics': 'MÃ©tricas de Performance',
                'strategic_recommendations': 'RecomendaÃ§Ãµes EstratÃ©gicas',
                'immediate_actions': 'AÃ§Ãµes Imediatas',
                'scale_assessment': 'AvaliaÃ§Ã£o de Escala',
                'project_types': 'Tipos de Projetos',
                'complexity_distribution': 'DistribuiÃ§Ã£o de Complexidade',
                'health_distribution': 'DistribuiÃ§Ã£o de SaÃºde',
                'efficiency_metrics': 'MÃ©tricas de EficiÃªncia',
                'risk_analysis': 'AnÃ¡lise de Risco',
                'optimization_opportunities': 'Oportunidades de OtimizaÃ§Ã£o',
                'ecosystem_health': 'SaÃºde do Ecossistema',
                'analysis_strategy': 'EstratÃ©gia de AnÃ¡lise',
                'data_extraction_quality': 'Qualidade da ExtraÃ§Ã£o de Dados',
                'insights_density': 'Densidade de Insights',
                'processing_efficiency': 'EficiÃªncia de Processamento',
                'memory_efficiency': 'EficiÃªncia de MemÃ³ria',
                'time_efficiency': 'EficiÃªncia de Tempo',
                'quality_score': 'PontuaÃ§Ã£o de Qualidade',
                'total_files': 'Total de Arquivos',
                'total_directories': 'Total de DiretÃ³rios',
                'total_projects': 'Total de Projetos',
                'binary_files': 'Arquivos BinÃ¡rios',
                'text_files': 'Arquivos de Texto',
                'broken_symlinks': 'Links SimbÃ³licos Quebrados',
                'inaccessible_files': 'Arquivos InacessÃ­veis',
                'large_files': 'Arquivos Grandes',
                'duplicate_files': 'Arquivos Duplicados',
                'empty_directories': 'DiretÃ³rios Vazios',
                'depth_analysis': 'AnÃ¡lise de Profundidade',
                'naming_conventions': 'ConvenÃ§Ãµes de Nomenclatura',
                'project_maturity': 'Maturidade do Projeto',
                'ecosystem_coordination': 'CoordenaÃ§Ã£o do Ecossistema',
                'patterns_detected': 'PadrÃµes Detectados',
                'anomalies_detected': 'Anomalias Detectadas',
                'recommendations': 'RecomendaÃ§Ãµes',
                'next_steps': 'PrÃ³ximos Passos',
                'critical_issues': 'Problemas CrÃ­ticos',
                'high_priority': 'Alta Prioridade',
                'medium_priority': 'MÃ©dia Prioridade',
                'low_priority': 'Baixa Prioridade',
                'improvements_needed': 'Melhorias NecessÃ¡rias',
                'strengths': 'ForÃ§as',
                'weaknesses': 'Fraquezas',
                'opportunities': 'Oportunidades',
                'threats': 'AmeaÃ§as',
                'score': 'PontuaÃ§Ã£o',
                'grade': 'ClassificaÃ§Ã£o',
                'status': 'Status',
                'excellent': 'Excelente',
                'good': 'Bom',
                'needs_attention': 'Precisa de AtenÃ§Ã£o',
                'critical': 'CrÃ­tico',
                'mega_ecosystem': 'Mega Ecossistema',
                'large_ecosystem': 'Grande Ecossistema',
                'medium_ecosystem': 'Ecossistema MÃ©dio',
                'small_ecosystem': 'Pequeno Ecossistema',
                'single_project': 'Projeto Ãšnico',
                'web': 'Web',
                'python': 'Python',
                'rust': 'Rust',
                'javascript': 'JavaScript',
                'typescript': 'TypeScript',
                'go': 'Go',
                'java': 'Java',
                'cpp': 'C++',
                'other': 'Outro',
                'high': 'Alto',
                'medium': 'MÃ©dio',
                'low': 'Baixo',
                'critical_severity': 'CrÃ­tico',
                'high_severity': 'Alto',
                'medium_severity': 'MÃ©dio',
                'low_severity': 'Baixo',
                'immediate': 'Imediato',
                'weeks': 'semanas',
                'days': 'dias',
                'hours': 'horas',
                'minutes': 'minutos',
                'seconds': 'segundos',
                'kb': 'KB',
                'mb': 'MB',
                'gb': 'GB',
                'tb': 'TB',
                'files': 'arquivos',
                'directories': 'diretÃ³rios',
                'projects': 'projetos',
                'levels': 'nÃ­veis',
                'groups': 'grupos',
                'guardrails': 'Limites de SeguranÃ§a',
                'project': 'Projeto',
                'analyzed': 'Analisado',
                'penalty_weight': 'Peso da Penalidade',
                'optimal_state': 'Estado Ã“timo',
                'current_gap': 'Gap Atual',
                'immediate_actions_required': 'AÃ§Ãµes Imediatas NecessÃ¡rias',
                'strategic_recommendations': 'RecomendaÃ§Ãµes EstratÃ©gicas',
                'conclusion': 'ConclusÃ£o',
                'analysis_completed': 'AnÃ¡lise ConcluÃ­da',
                'next_review': 'PrÃ³xima RevisÃ£o',
                'after_restructuring': 'ApÃ³s RestruturaÃ§Ã£o',
                'critical_issues_summary': 'Resumo de Problemas CrÃ­ticos',
                'project_grade': 'ClassificaÃ§Ã£o do Projeto',
                'recommendation': 'RecomendaÃ§Ã£o',
                'allocate_minimum': 'Alocar mÃ­nimo',
                'complete_project_restructuring': 'RestruturaÃ§Ã£o completa do projeto',
                'before_proceeding': 'Antes de prosseguir com qualquer novo desenvolvimento',
                'immediate_attention_and_major_restructuring': 'AtenÃ§Ã£o imediata e reestruturaÃ§Ã£o major',
                'requires_immediate_attention': 'Requer atenÃ§Ã£o imediata',
                'restructuring': 'RestruturaÃ§Ã£o',
            }
        }

    def t(self, key: str, lang: str = None) -> str:
        """Get translation"""
        if lang is None:
            lang = self.current_lang
        return self.translations.get(lang, {}).get(key, key)

    def start_memory_monitor(self):
        """Start memory monitoring thread"""
        def monitor():
            while not self.abort_analysis:
                try:
                    process = psutil.Process()
                    memory_gb = process.memory_info().rss / 1024 / 1024 / 1024

                    if memory_gb > self.MAX_MEMORY_GB:
                        print(f"âš ï¸ Memory limit exceeded: {memory_gb:.1f}GB")
                        self.abort_analysis = True

                    time.sleep(5)
                except:
                    break

        self.memory_monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.memory_monitor_thread.start()

    def check_time_limit(self) -> bool:
        """Check if we're approaching time limit"""
        if not self.start_time:
            return False

        elapsed = time.time() - self.start_time
        time_remaining = self.MAX_ANALYSIS_TIME - elapsed

        if time_remaining < 30:  # 30 seconds buffer
            print(f"â° Time limit approaching: {time_remaining:.1f}s remaining")
            return True

        return False

    def determine_analysis_strategy(self, surface_scan: dict) -> dict:
        """Deterministic decision tree for analysis strategy"""
        total_files = surface_scan.get('total_files', 0)
        total_projects = surface_scan.get('total_projects', 0)
        risk_factors = surface_scan.get('risk_factors', {})

        # Calculate risk score
        binary_ratio = risk_factors.get('binary_ratio', 0)
        broken_symlinks = risk_factors.get('broken_symlinks', 0)
        inaccessible_files = risk_factors.get('inaccessible_files', 0)
        large_files = risk_factors.get('large_files', 0)

        risk_score = (binary_ratio * 10) + (broken_symlinks * 0.1) + (inaccessible_files * 0.05) + (large_files * 0.01)

        # Deterministic strategy selection
        if total_projects == 1:
            if total_files < 500 and risk_score < 5:
                return {
                    'strategy': 'DEEP_ANALYSIS',
                    'confidence': 0.95,
                    'estimated_time': min(60, total_files * 0.1),
                    'sampling_rate': 1.0,
                    'plan_b_triggers': ['memory_limit', 'timeout', 'permission_denied']
                }
            elif total_files < 2000 and risk_score < 20:
                return {
                    'strategy': 'STANDARD_ANALYSIS',
                    'confidence': 0.85,
                    'estimated_time': min(120, total_files * 0.05),
                    'sampling_rate': 0.8,
                    'plan_b_triggers': ['memory_limit', 'timeout', 'high_binary_ratio']
                }
            else:
                return {
                    'strategy': 'SINGLE_PROJECT_PLAN_B',
                    'confidence': 0.75,
                    'estimated_time': min(90, total_files * 0.02),
                    'sampling_rate': 0.3,
                    'plan_b_triggers': ['memory_limit', 'timeout', 'corruption_detected']
                }

        elif total_projects < 10:
            if total_files < 5000 and risk_score < 15:
                return {
                    'strategy': 'SMALL_ECOSYSTEM_ANALYSIS',
                    'confidence': 0.8,
                    'estimated_time': min(150, total_files * 0.03),
                    'sampling_rate': 0.5,
                    'plan_b_triggers': ['memory_limit', 'timeout', 'ecosystem_too_complex']
                }
            else:
                return {
                    'strategy': 'ECOSYSTEM_PLAN_B',
                    'confidence': 0.7,
                    'estimated_time': min(120, total_projects * 2),
                    'sampling_rate': 0.1,
                    'plan_b_triggers': ['memory_limit', 'timeout', 'scale_overwhelming']
                }

        elif total_projects < 50:
            return {
                'strategy': 'LARGE_ECOSYSTEM_ANALYSIS',
                'confidence': 0.75,
                'estimated_time': min(180, total_projects * 1.5),
                'sampling_rate': 0.05,
                'plan_b_triggers': ['memory_limit', 'timeout', 'coordination_complexity']
            }

        else:  # Mega ecosystem
            return {
                'strategy': 'MEGA_ECOSYSTEM_STRATEGY',
                'confidence': 0.7,
                'estimated_time': min(180, total_projects * 1.0),
                'sampling_rate': 0.01,
                'plan_b_triggers': ['memory_limit', 'timeout', 'governance_breakdown']
            }

    def execute_plan_b_fallback(self, trigger: str, context: dict) -> dict:
        """Plan B fallback strategies"""
        fallback_strategies = {
            'memory_limit': {
                'name': 'Memory Management Mode',
                'actions': [
                    'Switch to streaming analysis',
                    'Process files in chunks of 100',
                    'Clear in-memory caches',
                    'Use disk-based temporary storage'
                ],
                'sampling_adjustment': 0.5,
                'depth_reduction': 0.3
            },
            'timeout': {
                'name': 'Time Optimization Mode',
                'actions': [
                    'Increase sampling rate',
                    'Skip content analysis',
                    'Focus on metadata only',
                    'Generate extrapolated insights'
                ],
                'sampling_adjustment': 0.2,
                'depth_reduction': 0.5
            },
            'high_binary_ratio': {
                'name': 'Text-Only Analysis Mode',
                'actions': [
                    'Skip all binary files',
                    'Report binary statistics separately',
                    'Focus on code files only',
                    'Analyze file extensions patterns'
                ],
                'sampling_adjustment': 2.0,  # Sample more text files
                'depth_reduction': 0.2
            },
            'broken_symlinks': {
                'name': 'Infrastructure Analysis Mode',
                'actions': [
                    'Skip broken symlinks',
                    'Count as infrastructure debt',
                    'Analyze symlink patterns',
                    'Focus on accessible content'
                ],
                'sampling_adjustment': 1.2,
                'depth_reduction': 0.1
            },
            'permission_denied': {
                'name': 'Partial Access Mode',
                'actions': [
                    'Log inaccessible areas',
                    'Focus on accessible directories',
                    'Report access restrictions',
                    'Analyze available data intensively'
                ],
                'sampling_adjustment': 1.5,
                'depth_reduction': 0.15
            },
            'scale_overwhelming': {
                'name': 'Statistical Sampling Mode',
                'actions': [
                    'Use representative sampling',
                    'Apply statistical extrapolation',
                    'Focus on patterns not details',
                    'Generate confidence intervals'
                ],
                'sampling_adjustment': 0.05,
                'depth_reduction': 0.7
            },
            'corruption_detected': {
                'name': 'Data Recovery Mode',
                'actions': [
                    'Skip corrupted files',
                    'Focus on readable content',
                    'Report corruption statistics',
                    'Salvage analyzable data'
                ],
                'sampling_adjustment': 0.8,
                'depth_reduction': 0.4
            }
        }

        strategy = fallback_strategies.get(trigger, fallback_strategies['timeout'])

        print(f"ðŸ”„ Plan B Activated: {strategy['name']}")
        for action in strategy['actions']:
            print(f"   â€¢ {action}")

        return {
            'trigger': trigger,
            'strategy': strategy,
            'context': context,
            'adaptation_success': True
        }

    def perform_maximum_extraction_analysis(self):
        """Main maximum information extraction analysis"""
        print("ðŸš€ Starting MAXIMUM INFORMATION EXTRACTION...")
        self.start_time = time.time()

        # Start monitoring
        self.start_memory_monitor()

        try:
            # Stage 1: Surface scan (always optimized)
            print("ðŸ” Stage 1: Surface Mapping...")
            self.surface_scan = self.perform_optimized_surface_scan()

            # Stage 2: Strategy determination
            print("ðŸ§  Stage 2: Strategy Determination...")
            strategy = self.determine_analysis_strategy(self.surface_scan)
            print(f"ðŸ“‹ Strategy: {strategy['strategy']} (Confidence: {strategy['confidence']:.0%})")

            # Stage 3: Adaptive execution
            print("âš¡ Stage 3: Adaptive Execution...")
            analysis_results = self.execute_adaptive_analysis(strategy)

            # Stage 4: Maximum insight generation
            print("ðŸ’¡ Stage 4: Maximum Insight Generation...")
            self.ecosystem_intelligence = self.generate_maximum_insights(
                self.surface_scan, analysis_results, strategy
            )

            # Stage 5: Advanced Analysis (NEW!)
            print("ðŸ”¬ Stage 5: Advanced Analysis...")
            self.run_advanced_analysis()

            # Performance metrics
            total_time = time.time() - self.start_time
            self.performance_metrics = self.calculate_performance_metrics(total_time)

            print(f"âœ… Maximum extraction complete in {total_time:.1f}s!")

        except Exception as e:
            print(f"âŒ Analysis error: {e}")
            # Emergency fallback
            self.execute_emergency_fallback(e)

    def perform_optimized_surface_scan(self) -> dict:
        """High-performance surface scan with maximum data extraction"""
        scan_start = time.time()

        # Use parallel processing for speed
        project_data = {}
        total_stats = {
            'total_files': 0,
            'total_directories': 0,
            'total_projects': 0,
            'project_types': defaultdict(int),
            'size_distribution': defaultdict(int),
            'complexity_distribution': defaultdict(int),
            'risk_factors': {
                'binary_files': 0,
                'text_files': 0,
                'broken_symlinks': 0,
                'inaccessible_files': 0,
                'large_files': 0,
                'binary_ratio': 0,
                'corrupted_files': 0
            },
            'file_types': defaultdict(int),
            'depth_analysis': {'max_depth': 0, 'avg_depth': 0, 'depth_distribution': defaultdict(int)},
            'performance_indicators': {
                'scan_rate': 0,
                'memory_efficiency': 0,
                'error_rate': 0
            }
        }

        # Process items in parallel
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = {}

            for item in self.project_path.iterdir():
                if item.is_dir() and not item.name.startswith('.'):
                    total_stats['total_projects'] += 1
                    future = executor.submit(self.scan_project_optimized, item)
                    futures[future] = item.name

            # Collect results
            for future in as_completed(futures):
                if self.abort_analysis or self.check_time_limit():
                    break

                project_name = futures[future]
                try:
                    project_info = future.result(timeout=30)  # 30s timeout per project
                    project_data[project_name] = project_info

                    # Aggregate statistics
                    self.aggregate_project_stats(project_info, total_stats, project_name)

                except Exception as e:
                    print(f"âš ï¸ Error scanning {project_name}: {e}")
                    total_stats['risk_factors']['inaccessible_files'] += 1

        # Calculate derived metrics
        total_files = total_stats['total_files']
        if total_files > 0:
            total_stats['risk_factors']['binary_ratio'] = (
                total_stats['risk_factors']['binary_files'] / total_files * 100
            )

        scan_time = time.time() - scan_start
        total_stats['performance_indicators']['scan_rate'] = total_files / scan_time if scan_time > 0 else 0

        project_data['summary'] = total_stats
        project_data['scan_metadata'] = {
            'scan_time': scan_time,
            'projects_analyzed': total_stats['total_projects'],
            'files_discovered': total_stats['total_files'],
            'scan_strategy': 'parallel_optimized'
        }

        print(f"ðŸ“Š Surface scan: {total_stats['total_projects']} projects, {total_stats['total_files']} files ({scan_time:.1f}s)")
        return project_data

    def scan_project_optimized(self, project_path: Path) -> dict:
        """Optimized project scanning with risk assessment"""
        project_info = {
            'file_count': 0,
            'directory_count': 0,
            'total_size': 0,
            'file_types': defaultdict(int),
            'indicators': [],
            'risk_factors': defaultdict(int),
            'complexity_metrics': {
                'max_depth': 0,
                'avg_file_size': 0,
                'has_package_manager': False,
                'has_documentation': False,
                'has_tests': False,
                'has_build_config': False
            },
            'health_indicators': {
                'has_readme': False,
                'has_license': False,
                'has_git': False,
                'has_ci_cd': False
            },
            'sample_files': []  # For deep analysis later
        }

        try:
            max_depth = 0
            file_sizes = []

            for root, dirs, files in os.walk(project_path, followlinks=False):
                # Calculate depth
                current_depth = root.count(os.sep) - str(project_path).count(os.sep)
                max_depth = max(max_depth, current_depth)

                # Skip obviously large directories for surface scan
                dirs[:] = [d for d in dirs if d not in [
                    'node_modules', '__pycache__', '.git', 'dist', 'build',
                    'target', 'vendor', '.next', '.nuxt', 'coverage', 'site-packages'
                ]]
                project_info['directory_count'] += len(dirs)

                for file in files[:200]:  # Limit per project for surface scan
                    if self.abort_analysis:
                        break

                    file_path = Path(root) / file

                    try:
                        # Quick stats
                        project_info['file_count'] += 1

                        # File type detection
                        ext = file_path.suffix.lower()
                        project_info['file_types'][ext] += 1

                        # Risk assessment
                        if ext in ['.jpg', '.jpeg', '.png', '.gif', '.heic', '.mov', '.mp4', '.avi', '.zip', '.tar.gz']:
                            project_info['risk_factors']['binary_files'] += 1
                        elif ext in ['.py', '.js', '.ts', '.jsx', '.tsx', '.md', '.txt', '.json', '.yml', '.yaml', '.html', '.css']:
                            project_info['risk_factors']['text_files'] += 1

                        # Indicator files
                        if file in ['package.json', 'requirements.txt', 'Cargo.toml', 'setup.py', 'go.mod', 'pom.xml']:
                            project_info['indicators'].append(file)
                            project_info['complexity_metrics']['has_package_manager'] = True

                        elif file in ['README.md', 'README.txt', 'README']:
                            project_info['indicators'].append(file)
                            project_info['health_indicators']['has_readme'] = True
                            project_info['complexity_metrics']['has_documentation'] = True

                        elif file in ['LICENSE', 'LICENSE.txt', 'LICENSE.md']:
                            project_info['health_indicators']['has_license'] = True

                        elif file in ['Dockerfile', 'docker-compose.yml', 'Dockerfile.prod']:
                            project_info['indicators'].append(file)
                            project_info['complexity_metrics']['has_build_config'] = True

                        # Sample collection for potential deep analysis
                        if len(project_info['sample_files']) < 10:
                            if ext in ['.py', '.js', '.ts', '.tsx', '.jsx', '.md', '.json']:
                                project_info['sample_files'].append(str(file_path))

                        # File size analysis
                        if file_path.exists():
                            stat = file_path.stat()
                            file_size = stat.st_size
                            project_info['total_size'] += file_size
                            file_sizes.append(file_size)

                            if file_size > 100_000_000:  # 100MB
                                project_info['risk_factors']['large_files'] += 1

                    except (OSError, PermissionError):
                        project_info['risk_factors']['inaccessible_files'] += 1

                    except Exception:
                        project_info['risk_factors']['corrupted_files'] += 1

            # Calculate complexity metrics
            project_info['complexity_metrics']['max_depth'] = max_depth
            if file_sizes:
                project_info['complexity_metrics']['avg_file_size'] = sum(file_sizes) / len(file_sizes)

            # Git detection
            if (project_path / '.git').exists():
                project_info['health_indicators']['has_git'] = True

            # CI/CD detection
            for ci_file in ['.github', '.gitlab-ci.yml', 'Jenkinsfile', '.travis.yml', 'circle.yml']:
                if (project_path / ci_file).exists():
                    project_info['health_indicators']['has_ci_cd'] = True
                    break

        except Exception as e:
            project_info['scan_error'] = str(e)
            project_info['risk_factors']['inaccessible_files'] += 100  # Mark as problematic

        return project_info

    def aggregate_project_stats(self, project_info: dict, total_stats: dict, project_name: str):
        """Aggregate project statistics into total"""
        total_stats['total_files'] += project_info.get('file_count', 0)
        total_stats['total_directories'] += project_info.get('directory_count', 0)

        # Project type classification
        indicators = project_info.get('indicators', [])
        if 'package.json' in indicators or any(f.endswith('.js') or f.endswith('.ts') for f in indicators):
            total_stats['project_types']['web'] += 1
        elif 'requirements.txt' in indicators or 'setup.py' in indicators or 'go.mod' in indicators:
            total_stats['project_types']['python'] += 1
        elif 'Cargo.toml' in indicators:
            total_stats['project_types']['rust'] += 1
        elif 'pom.xml' in indicators:
            total_stats['project_types']['java'] += 1
        else:
            total_stats['project_types']['other'] += 1

        # Size categorization
        file_count = project_info.get('file_count', 0)
        if file_count < 100:
            total_stats['size_distribution']['small'] += 1
        elif file_count < 500:
            total_stats['size_distribution']['medium'] += 1
        elif file_count < 2000:
            total_stats['size_distribution']['large'] += 1
        else:
            total_stats['size_distribution']['mega'] += 1

        # Complexity distribution
        complexity = project_info.get('complexity_metrics', {})
        max_depth = complexity.get('max_depth', 0)
        if max_depth < 3:
            total_stats['complexity_distribution']['simple'] += 1
        elif max_depth < 6:
            total_stats['complexity_distribution']['moderate'] += 1
        else:
            total_stats['complexity_distribution']['complex'] += 1

        # Aggregate risk factors
        risk_factors = project_info.get('risk_factors', {})
        for key, value in risk_factors.items():
            if key in total_stats['risk_factors']:
                total_stats['risk_factors'][key] += value

        # Aggregate file types
        file_types = project_info.get('file_types', {})
        for ext, count in file_types.items():
            total_stats['file_types'][ext] += count

        # Depth analysis
        depth = complexity.get('max_depth', 0)
        total_stats['depth_analysis']['max_depth'] = max(total_stats['depth_analysis']['max_depth'], depth)
        total_stats['depth_analysis']['depth_distribution'][depth] += 1

    def execute_adaptive_analysis(self, strategy: dict) -> dict:
        """Execute analysis based on determined strategy"""
        strategy_name = strategy['strategy']
        sampling_rate = strategy['sampling_rate']

        print(f"ðŸŽ¯ Executing: {strategy_name}")
        print(f"ðŸ“Š Sampling rate: {sampling_rate:.0%}")

        analysis_results = {
            'strategy_used': strategy_name,
            'sampling_rate': sampling_rate,
            'confidence': strategy['confidence'],
            'plan_b_activations': [],
            'data_extracted': {
                'files_analyzed': 0,
                'content_analyzed': 0,
                'patterns_detected': 0,
                'insights_generated': 0
            }
        }

        try:
            if strategy_name == 'DEEP_ANALYSIS':
                analysis_results.update(self.perform_deep_analysis(sampling_rate))

            elif strategy_name == 'STANDARD_ANALYSIS':
                analysis_results.update(self.perform_standard_analysis(sampling_rate))

            elif strategy_name == 'SMALL_ECOSYSTEM_ANALYSIS':
                analysis_results.update(self.perform_ecosystem_analysis(sampling_rate, 'small'))

            elif strategy_name in ['SINGLE_PROJECT_PLAN_B', 'ECOSYSTEM_PLAN_B']:
                analysis_results.update(self.perform_plan_b_analysis(sampling_rate, strategy_name))

            elif strategy_name in ['LARGE_ECOSYSTEM_ANALYSIS', 'MEGA_ECOSYSTEM_STRATEGY']:
                analysis_results.update(self.perform_ecosystem_analysis(sampling_rate, 'large'))

        except Exception as e:
            # Trigger Plan B
            trigger = 'timeout' if 'time' in str(e).lower() else 'corruption_detected'
            plan_b = self.execute_plan_b_fallback(trigger, {'error': str(e)})
            analysis_results['plan_b_activations'].append(plan_b)

            # Try simplified analysis
            analysis_results.update(self.perform_emergency_analysis())

        return analysis_results

    def perform_deep_analysis(self, sampling_rate: float) -> dict:
        """Deep file analysis for small projects"""
        print("ðŸ”¬ Deep analysis mode...")

        results = {
            'analysis_type': 'deep',
            'files_analyzed': 0,
            'content_analyzed': 0,
            'patterns_found': [],
            'duplicates_detected': [],
            'naming_issues': [],
            'quality_metrics': {}
        }

        # Analyze all projects with sampling
        for project_name, project_info in self.surface_scan.items():
            if project_name == 'summary':
                continue

            if self.abort_analysis or self.check_time_limit():
                break

            project_path = self.project_path / project_name
            if not project_path.exists():
                continue

            # Analyze sample files
            sample_files = project_info.get('sample_files', [])
            files_to_analyze = sample_files[:max(1, int(len(sample_files) * sampling_rate))]

            for file_path_str in files_to_analyze:
                if self.abort_analysis:
                    break

                try:
                    file_path = Path(file_path_str)
                    if not file_path.exists():
                        continue

                    # Read and analyze content
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read(10240)  # Read first 10KB

                    results['files_analyzed'] += 1
                    results['content_analyzed'] += len(content)

                    # Pattern detection
                    patterns = self.detect_patterns(content, file_path.suffix)
                    results['patterns_found'].extend(patterns)

                    # Quality metrics
                    quality = self.analyze_code_quality(content, file_path.suffix)
                    if quality:
                        results['quality_metrics'][str(file_path)] = quality

                except Exception as e:
                    continue

        # Analyze patterns
        results['patterns_detected'] = len(set(results['patterns_found']))

        return results

    def perform_standard_analysis(self, sampling_rate: float) -> dict:
        """Standard analysis for medium projects"""
        print("ðŸ“‹ Standard analysis mode...")

        results = {
            'analysis_type': 'standard',
            'files_analyzed': 0,
            'projects_analyzed': 0,
            'health_scores': {},
            'complexity_metrics': {},
            'risk_assessment': {}
        }

        for project_name, project_info in self.surface_scan.items():
            if project_name == 'summary':
                continue

            if self.abort_analysis or self.check_time_limit():
                break

            results['projects_analyzed'] += 1

            # Health scoring based on surface scan
            health_score = self.calculate_project_health(project_info)
            results['health_scores'][project_name] = health_score

            # Complexity analysis
            complexity = self.assess_project_complexity(project_info)
            results['complexity_metrics'][project_name] = complexity

            # Risk assessment
            risk = self.assess_project_risks(project_info)
            results['risk_assessment'][project_name] = risk

        return results

    def perform_ecosystem_analysis(self, sampling_rate: float, ecosystem_size: str) -> dict:
        """Ecosystem-level analysis for large project collections"""
        print(f"ðŸŒ Ecosystem analysis mode ({ecosystem_size})...")

        results = {
            'analysis_type': 'ecosystem',
            'ecosystem_size': ecosystem_size,
            'projects_analyzed': 0,
            'ecosystem_health': 0,
            'diversity_metrics': {},
            'coordination_assessment': {},
            'patterns_detected': []
        }

        project_health_scores = []
        project_types = []
        complexity_scores = []

        # Process projects with sampling
        projects_to_analyze = list(self.surface_scan.keys())
        if 'summary' in projects_to_analyze:
            projects_to_analyze.remove('summary')

        # Apply sampling for large ecosystems
        if ecosystem_size == 'large':
            sample_size = max(1, int(len(projects_to_analyze) * sampling_rate))
            projects_to_analyze = projects_to_analyze[:sample_size]

        for project_name in projects_to_analyze:
            if self.abort_analysis or self.check_time_limit():
                break

            project_info = self.surface_scan.get(project_name, {})
            if not project_info:
                continue

            results['projects_analyzed'] += 1

            # Health scoring
            health = self.calculate_project_health(project_info)
            project_health_scores.append(health)

            # Type classification
            project_type = self.classify_project_type(project_info)
            project_types.append(project_type)

            # Complexity scoring
            complexity = self.assess_project_complexity(project_info)
            complexity_scores.append(complexity.get('overall_score', 0))

        # Calculate ecosystem metrics
        if project_health_scores:
            results['ecosystem_health'] = sum(project_health_scores) / len(project_health_scores)

        # Diversity analysis
        type_counter = Counter(project_types)
        results['diversity_metrics'] = {
            'shannon_diversity': self.calculate_shannon_diversity(type_counter),
            'dominant_type': type_counter.most_common(1)[0] if type_counter else ('unknown', 0),
            'type_distribution': dict(type_counter)
        }

        # Coordination assessment
        results['coordination_assessment'] = self.assess_ecosystem_coordination(
            self.surface_scan, projects_to_analyze
        )

        # Pattern detection
        results['patterns_detected'] = self.detect_ecosystem_patterns(
            self.surface_scan, projects_to_analyze
        )

        return results

    def perform_plan_b_analysis(self, sampling_rate: float, strategy_name: str) -> dict:
        """Plan B analysis for challenging scenarios"""
        print(f"ðŸ”„ Plan B analysis mode ({strategy_name})...")

        results = {
            'analysis_type': 'plan_b',
            'strategy_name': strategy_name,
            'adaptations_made': [],
            'data_extracted': {
                'surface_metrics': True,
                'sampled_content': False,
                'pattern_detection': False,
                'statistical_extrapolation': True
            },
            'confidence_adjusted': 0.5
        }

        try:
            # Extract maximum value from surface scan
            summary = self.surface_scan.get('summary', {})

            # Statistical extrapolation
            total_projects = summary.get('total_projects', 0)
            total_files = summary.get('total_files', 0)

            if total_projects > 0:
                # Extrapolate from sample
                sample_size = max(5, min(20, int(total_projects * sampling_rate)))
                sample_projects = list(self.surface_scan.keys())[:sample_size]

                if 'summary' in sample_projects:
                    sample_projects.remove('summary')

                # Analyze sample intensively
                sample_health = []
                sample_complexity = []

                for project_name in sample_projects:
                    project_info = self.surface_scan.get(project_name, {})
                    if project_info:
                        health = self.calculate_project_health(project_info)
                        sample_health.append(health)

                        complexity = self.assess_project_complexity(project_info)
                        sample_complexity.append(complexity.get('overall_score', 0))

                # Extrapolate to full ecosystem
                if sample_health:
                    avg_health = sum(sample_health) / len(sample_health)
                    results['extrapolated_health'] = avg_health
                    results['confidence_adjusted'] = min(0.8, len(sample_health) / total_projects)

                results['sample_analysis'] = {
                    'projects_sampled': len(sample_projects),
                    'avg_health': sum(sample_health) / len(sample_health) if sample_health else 0,
                    'avg_complexity': sum(sample_complexity) / len(sample_complexity) if sample_complexity else 0
                }

            results['adaptations_made'].append('Statistical extrapolation from limited sample')
            results['adaptations_made'].append('Surface-only analysis with confidence intervals')

        except Exception as e:
            results['adaptations_made'].append(f'Emergency fallback: {str(e)}')
            results['data_extracted'] = {
                'surface_metrics': True,
                'sampled_content': False,
                'pattern_detection': False,
                'statistical_extrapolation': False,
                'emergency_only': True
            }

        return results

    def perform_emergency_analysis(self) -> dict:
        """Emergency analysis when all else fails"""
        print("ðŸš¨ Emergency analysis mode...")

        summary = self.surface_scan.get('summary', {})

        return {
            'analysis_type': 'emergency',
            'data_extracted': {
                'total_projects': summary.get('total_projects', 0),
                'total_files': summary.get('total_files', 0),
                'project_types': dict(summary.get('project_types', {})),
                'risk_factors': summary.get('risk_factors', {}),
                'size_distribution': dict(summary.get('size_distribution', {}))
            },
            'confidence': 0.3,
            'adaptations_made': ['Emergency fallback - surface scan only'],
            'recommendations': [
                'Project requires manual investigation',
                'Consider breaking into smaller sub-projects',
                'Implement automated monitoring and analysis'
            ]
        }

    def execute_emergency_fallback(self, error: Exception):
        """Ultimate fallback when analysis fails"""
        print(f"ðŸš¨ Emergency fallback activated: {error}")

        # Set minimal results for report generation
        self.surface_scan = {
            'summary': {
                'total_projects': 0,
                'total_files': 0,
                'project_types': {},
                'risk_factors': {'analysis_failed': True},
                'scan_metadata': {'error': str(error)}
            }
        }

        self.ecosystem_intelligence = {
            'error_occurred': True,
            'error_message': str(error),
            'fallback_level': 'emergency',
            'recommendations': [
                'Analysis encountered critical errors',
                'Manual investigation required',
                'Consider reducing analysis scope'
            ]
        }

    def detect_patterns(self, content: str, file_extension: str) -> list:
        """Detect patterns in file content"""
        patterns = []

        try:
            # Language-specific patterns
            if file_extension in ['.py', '.pyx', '.pyi']:
                # Python patterns
                if re.search(r'class\s+\w+', content):
                    patterns.append('class_definition')
                if re.search(r'def\s+\w+', content):
                    patterns.append('function_definition')
                if re.search(r'import\s+\w+', content):
                    patterns.append('import_statement')
                if re.search(r'from\s+\w+\s+import', content):
                    patterns.append('from_import')
                if re.search(r'if\s+__name__\s*==\s*["\']__main__["\']', content):
                    patterns.append('main_guard')

            elif file_extension in ['.js', '.jsx', '.ts', '.tsx']:
                # JavaScript/TypeScript patterns
                if re.search(r'function\s+\w+|=>\s*{', content):
                    patterns.append('function_definition')
                if re.search(r'const\s+\w+\s*=|let\s+\w+\s*=|var\s+\w+\s*=', content):
                    patterns.append('variable_declaration')
                if re.search(r'import\s+.*from', content):
                    patterns.append('import_statement')
                if re.search(r'export\s+', content):
                    patterns.append('export_statement')
                if re.search(r'class\s+\w+', content):
                    patterns.append('class_definition')

            elif file_extension == '.json':
                # JSON patterns
                try:
                    json.loads(content)
                    patterns.append('valid_json')
                except:
                    patterns.append('invalid_json')

            elif file_extension in ['.yml', '.yaml']:
                # YAML patterns
                if re.search(r'^\s*\w+\s*:', content, re.MULTILINE):
                    patterns.append('yaml_structure')
                if re.search(r'^\s*-\s+', content, re.MULTILINE):
                    patterns.append('yaml_list')

            # Generic patterns
            if re.search(r'//TODO|//FIXME|#TODO|#FIXME|TODO:|FIXME:', content):
                patterns.append('todo_comments')
            if re.search(r'http[s]?://\w+', content):
                patterns.append('urls_present')
            if re.search(r'\w+@\w+\.\w+', content):
                patterns.append('email_present')

        except Exception:
            pass

        return patterns

    def analyze_code_quality(self, content: str, file_extension: str) -> dict:
        """Basic code quality analysis"""
        quality = {
            'line_count': len(content.splitlines()),
            'char_count': len(content),
            'has_comments': False,
            'has_documentation': False,
            'complexity_estimate': 'low'
        }

        try:
            # Comment detection
            if file_extension in ['.py', '.pyx', '.pyi']:
                if re.search(r'#.*', content):
                    quality['has_comments'] = True
                if re.search(r'""".*?"""', content, re.DOTALL):
                    quality['has_documentation'] = True

            elif file_extension in ['.js', '.jsx', '.ts', '.tsx']:
                if re.search(r'//.*|/\*.*?\*/', content):
                    quality['has_comments'] = True
                if re.search(r'/\*\*.*?\*/', content, re.DOTALL):
                    quality['has_documentation'] = True

            # Complexity estimation
            line_count = quality['line_count']
            if line_count > 500:
                quality['complexity_estimate'] = 'high'
            elif line_count > 100:
                quality['complexity_estimate'] = 'medium'

        except Exception:
            pass

        return quality

    def calculate_project_health(self, project_info: dict) -> float:
        """Calculate health score for individual project"""
        health_score = 50.0  # Base score

        try:
            # Positive indicators
            indicators = project_info.get('indicators', [])
            health_indicators = project_info.get('health_indicators', {})

            if health_indicators.get('has_readme'):
                health_score += 15
            if health_indicators.get('has_license'):
                health_score += 10
            if health_indicators.get('has_git'):
                health_score += 10
            if health_indicators.get('has_ci_cd'):
                health_score += 15

            # Package manager presence
            if project_info.get('complexity_metrics', {}).get('has_package_manager'):
                health_score += 10

            # Documentation presence
            if project_info.get('complexity_metrics', {}).get('has_documentation'):
                health_score += 10

            # File count considerations
            file_count = project_info.get('file_count', 0)
            if file_count > 5:
                health_score += 5
            elif file_count < 3:
                health_score -= 15

            # Risk factors
            risk_factors = project_info.get('risk_factors', {})
            if risk_factors.get('inaccessible_files', 0) > 10:
                health_score -= 20
            if risk_factors.get('corrupted_files', 0) > 5:
                health_score -= 15
            if risk_factors.get('broken_symlinks', 0) > 5:
                health_score -= 10

            # Size considerations
            total_size = project_info.get('total_size', 0)
            if total_size > 1_000_000_000:  # 1GB
                health_score -= 10

            # Clamp to valid range
            health_score = max(0, min(100, health_score))

        except Exception:
            health_score = 25.0  # Minimal score on error

        return health_score

    def assess_project_complexity(self, project_info: dict) -> dict:
        """Assess project complexity"""
        complexity = {
            'structural_complexity': 'low',
            'dependency_complexity': 'low',
            'file_complexity': 'low',
            'overall_score': 0
        }

        try:
            score = 0

            # Structural complexity
            complexity_metrics = project_info.get('complexity_metrics', {})
            max_depth = complexity_metrics.get('max_depth', 0)
            directory_count = project_info.get('directory_count', 0)

            if max_depth > 8 or directory_count > 50:
                complexity['structural_complexity'] = 'high'
                score += 30
            elif max_depth > 4 or directory_count > 15:
                complexity['structural_complexity'] = 'medium'
                score += 20
            else:
                score += 10

            # File complexity
            file_count = project_info.get('file_count', 0)
            file_types = len(project_info.get('file_types', {}))

            if file_count > 1000 or file_types > 20:
                complexity['file_complexity'] = 'high'
                score += 30
            elif file_count > 100 or file_types > 10:
                complexity['file_complexity'] = 'medium'
                score += 20
            else:
                score += 10

            # Dependency complexity
            indicators = project_info.get('indicators', [])
            if 'package.json' in indicators or 'requirements.txt' in indicators:
                complexity['dependency_complexity'] = 'medium'
                score += 15

            if 'docker-compose.yml' in indicators or 'Dockerfile' in indicators:
                complexity['dependency_complexity'] = 'high'
                score += 25

            complexity['overall_score'] = min(100, score)

        except Exception:
            complexity['overall_score'] = 25

        return complexity

    def assess_project_risks(self, project_info: dict) -> dict:
        """Assess project risks"""
        risks = {
            'security_risks': [],
            'maintenance_risks': [],
            'scalability_risks': [],
            'overall_risk_level': 'low'
        }

        try:
            risk_score = 0
            risk_factors = project_info.get('risk_factors', {})

            # Security risks
            if risk_factors.get('inaccessible_files', 0) > 20:
                risks['security_risks'].append('High number of inaccessible files')
                risk_score += 20

            # Maintenance risks
            if not project_info.get('health_indicators', {}).get('has_documentation'):
                risks['maintenance_risks'].append('No documentation detected')
                risk_score += 15

            if risk_factors.get('corrupted_files', 0) > 0:
                risks['maintenance_risks'].append('Corrupted files present')
                risk_score += 10

            # Scalability risks
            if risk_factors.get('large_files', 0) > 5:
                risks['scalability_risks'].append('Multiple large files present')
                risk_score += 15

            total_size = project_info.get('total_size', 0)
            if total_size > 2_000_000_000:  # 2GB
                risks['scalability_risks'].append('Very large project size')
                risk_score += 20

            # Determine overall risk level
            if risk_score > 50:
                risks['overall_risk_level'] = 'high'
            elif risk_score > 25:
                risks['overall_risk_level'] = 'medium'
            else:
                risks['overall_risk_level'] = 'low'

        except Exception:
            risks['overall_risk_level'] = 'unknown'

        return risks

    def classify_project_type(self, project_info: dict) -> str:
        """Classify project type based on indicators"""
        indicators = project_info.get('indicators', [])
        file_types = project_info.get('file_types', {})

        # Web project detection
        if 'package.json' in indicators:
            return 'web'

        # Python project detection
        if 'requirements.txt' in indicators or 'setup.py' in indicators:
            return 'python'

        # Rust project detection
        if 'Cargo.toml' in indicators:
            return 'rust'

        # Go project detection
        if 'go.mod' in indicators:
            return 'go'

        # Java project detection
        if 'pom.xml' in indicators:
            return 'java'

        # C++ project detection
        if file_types.get('.cpp', 0) > 0 or file_types.get('.cc', 0) > 0:
            return 'cpp'

        # TypeScript/JavaScript detection
        if file_types.get('.ts', 0) > 0 or file_types.get('.tsx', 0) > 0:
            return 'typescript'
        elif file_types.get('.js', 0) > 0 or file_types.get('.jsx', 0) > 0:
            return 'javascript'

        return 'other'

    def calculate_shannon_diversity(self, type_counts: Counter) -> float:
        """Calculate Shannon diversity index for project types"""
        try:
            total = sum(type_counts.values())
            if total == 0:
                return 0.0

            diversity = 0.0
            for count in type_counts.values():
                if count > 0:
                    proportion = count / total
                    diversity -= proportion * math.log(proportion)

            return diversity
        except Exception:
            return 0.0

    def assess_ecosystem_coordination(self, surface_scan: dict, projects_analyzed: list) -> dict:
        """Assess coordination across ecosystem"""
        coordination = {
            'standardization_score': 0,
            'common_patterns': [],
            'coordination_issues': [],
            'recommendations': []
        }

        try:
            # Look for common patterns across projects
            common_indicators = defaultdict(int)
            health_scores = []

            for project_name in projects_analyzed:
                project_info = surface_scan.get(project_name, {})
                if not project_info:
                    continue

                # Health tracking
                health = self.calculate_project_health(project_info)
                health_scores.append(health)

                # Indicator tracking
                indicators = project_info.get('indicators', [])
                for indicator in indicators:
                    common_indicators[indicator] += 1

            # Calculate standardization score
            total_projects = len(projects_analyzed)
            if total_projects > 0:
                standardization = 0

                # Common tools and practices
                if common_indicators.get('README.md', 0) / total_projects > 0.7:
                    standardization += 25
                    coordination['common_patterns'].append('High documentation coverage')

                if common_indicators.get('LICENSE', 0) / total_projects > 0.5:
                    standardization += 20
                    coordination['common_patterns'].append('Good licensing practices')

                if common_indicators.get('Dockerfile', 0) / total_projects > 0.3:
                    standardization += 20
                    coordination['common_patterns'].append('Containerization adoption')

                # Health consistency
                if health_scores:
                    avg_health = sum(health_scores) / len(health_scores)
                    health_variance = sum((h - avg_health) ** 2 for h in health_scores) / len(health_scores)

                    if health_variance < 100:  # Low variance = consistent
                        standardization += 15
                        coordination['common_patterns'].append('Consistent project health')

                coordination['standardization_score'] = min(100, standardization)

            # Coordination issues
            if common_indicators.get('README.md', 0) / total_projects < 0.3:
                coordination['coordination_issues'].append('Poor documentation coverage')

            if health_scores and min(health_scores) < 30:
                coordination['coordination_issues'].append('Very unhealthy projects present')

            # Recommendations
            if coordination['standardization_score'] < 50:
                coordination['recommendations'].append('Implement ecosystem-wide standards')

            if coordination['coordination_issues']:
                coordination['recommendations'].append('Address coordination issues proactively')

        except Exception as e:
            coordination['error'] = str(e)

        return coordination

    def detect_ecosystem_patterns(self, surface_scan: dict, projects_analyzed: list) -> list:
        """Detect patterns across ecosystem"""
        patterns = []

        try:
            # Size patterns
            sizes = []
            for project_name in projects_analyzed:
                project_info = surface_scan.get(project_name, {})
                if project_info:
                    sizes.append(project_info.get('file_count', 0))

            if sizes:
                avg_size = sum(sizes) / len(sizes)
                if avg_size > 1000:
                    patterns.append('Ecosystem consists of large projects')
                elif avg_size < 50:
                    patterns.append('Ecosystem consists of small projects')

            # Type distribution patterns
            type_counts = defaultdict(int)
            for project_name in projects_analyzed:
                project_info = surface_scan.get(project_name, {})
                if project_info:
                    project_type = self.classify_project_type(project_info)
                    type_counts[project_type] += 1

            if type_counts:
                dominant_type, count = max(type_counts.items(), key=lambda x: x[1])
                if count / len(projects_analyzed) > 0.6:
                    patterns.append(f'Dominant technology: {dominant_type}')

            # Maturity patterns
            healthy_projects = 0
            for project_name in projects_analyzed:
                project_info = surface_scan.get(project_name, {})
                if project_info:
                    health = self.calculate_project_health(project_info)
                    if health > 60:
                        healthy_projects += 1

            if healthy_projects / len(projects_analyzed) > 0.7:
                patterns.append('Mature ecosystem with good practices')
            elif healthy_projects / len(projects_analyzed) < 0.3:
                patterns.append('Immature ecosystem needs improvement')

        except Exception:
            pass

        return patterns

    def generate_maximum_insights(self, surface_scan: dict, analysis_results: dict, strategy: dict) -> dict:
        """Generate maximum insights from all analysis data"""
        print("ðŸ§  Generating maximum insights...")

        insights = {
            'scale_assessment': {},
            'ecosystem_intelligence': {},
            'performance_metrics': {},
            'strategic_recommendations': [],
            'immediate_actions': [],
            'risk_analysis': {},
            'optimization_opportunities': [],
            'quality_assessment': {},
            'meta_analysis': {}
        }

        try:
            summary = surface_scan.get('summary', {})

            # Scale assessment
            total_projects = summary.get('total_projects', 0)
            total_files = summary.get('total_files', 0)

            if total_projects == 1:
                scale_category = 'single_project'
            elif total_projects < 10:
                scale_category = 'small_ecosystem'
            elif total_projects < 50:
                scale_category = 'medium_ecosystem'
            elif total_projects < 100:
                scale_category = 'large_ecosystem'
            else:
                scale_category = 'mega_ecosystem'

            insights['scale_assessment'] = {
                'category': scale_category,
                'total_projects': total_projects,
                'total_files': total_files,
                'complexity_level': self.assess_overall_complexity(summary),
                'maturity_level': self.assess_overall_maturity(analysis_results)
            }

            # Ecosystem intelligence
            if analysis_results.get('analysis_type') == 'ecosystem':
                insights['ecosystem_intelligence'] = {
                    'ecosystem_health': analysis_results.get('ecosystem_health', 0),
                    'diversity_metrics': analysis_results.get('diversity_metrics', {}),
                    'coordination_assessment': analysis_results.get('coordination_assessment', {}),
                    'patterns_detected': analysis_results.get('patterns_detected', [])
                }

            # Performance metrics
            scan_metadata = surface_scan.get('scan_metadata', {})
            insights['performance_metrics'] = {
                'scan_time': scan_metadata.get('scan_time', 0),
                'scan_rate': summary.get('performance_indicators', {}).get('scan_rate', 0),
                'analysis_strategy': strategy.get('strategy', 'unknown'),
                'confidence_level': strategy.get('confidence', 0),
                'sampling_efficiency': self.calculate_sampling_efficiency(analysis_results)
            }

            # Risk analysis
            risk_factors = summary.get('risk_factors', {})
            insights['risk_analysis'] = {
                'overall_risk_level': self.assess_overall_risk(risk_factors),
                'critical_risks': self.identify_critical_risks(risk_factors),
                'risk_mitigation_strategies': self.generate_risk_mitigation_strategies(risk_factors)
            }

            # Strategic recommendations
            insights['strategic_recommendations'] = self.generate_strategic_recommendations(
                insights['scale_assessment'],
                insights['ecosystem_intelligence'],
                insights['risk_analysis']
            )

            # Immediate actions
            insights['immediate_actions'] = self.generate_immediate_actions(
                insights['risk_analysis'],
                analysis_results
            )

            # Optimization opportunities
            insights['optimization_opportunities'] = self.identify_optimization_opportunities(
                surface_scan, analysis_results
            )

            # Quality assessment
            insights['quality_assessment'] = self.assess_overall_quality(
                surface_scan, analysis_results, strategy
            )

            # Meta analysis
            insights['meta_analysis'] = {
                'analysis_effectiveness': self.assess_analysis_effectiveness(analysis_results),
                'data_extraction_quality': self.assess_data_extraction_quality(analysis_results),
                'confidence_justification': self.explain_confidence_level(strategy, analysis_results),
                'limitations': self.identify_analysis_limitations(strategy, analysis_results)
            }

        except Exception as e:
            insights['generation_error'] = str(e)

        return insights

    def assess_overall_complexity(self, summary: dict) -> str:
        """Assess overall project complexity"""
        total_files = summary.get('total_files', 0)
        total_projects = summary.get('total_projects', 0)
        file_types = len(summary.get('file_types', {}))

        complexity_score = 0

        if total_files > 10000:
            complexity_score += 30
        elif total_files > 1000:
            complexity_score += 20
        elif total_files > 100:
            complexity_score += 10

        if total_projects > 50:
            complexity_score += 25
        elif total_projects > 10:
            complexity_score += 15
        elif total_projects > 5:
            complexity_score += 10

        if file_types > 20:
            complexity_score += 20
        elif file_types > 10:
            complexity_score += 15
        elif file_types > 5:
            complexity_score += 10

        if complexity_score > 60:
            return 'very_high'
        elif complexity_score > 40:
            return 'high'
        elif complexity_score > 20:
            return 'medium'
        else:
            return 'low'

    def assess_overall_maturity(self, analysis_results: dict) -> str:
        """Assess overall ecosystem maturity"""
        if analysis_results.get('analysis_type') == 'ecosystem':
            ecosystem_health = analysis_results.get('ecosystem_health', 0)
            coordination = analysis_results.get('coordination_assessment', {})
            standardization = coordination.get('standardization_score', 0)

            maturity_score = (ecosystem_health + standardization) / 2

            if maturity_score > 80:
                return 'very_mature'
            elif maturity_score > 60:
                return 'mature'
            elif maturity_score > 40:
                return 'developing'
            else:
                return 'immature'

        return 'unknown'

    def assess_overall_risk(self, risk_factors: dict) -> str:
        """Assess overall risk level"""
        risk_score = 0

        # File corruption risks
        corrupted = risk_factors.get('corrupted_files', 0)
        if corrupted > 10:
            risk_score += 30
        elif corrupted > 0:
            risk_score += 15

        # Access risks
        inaccessible = risk_factors.get('inaccessible_files', 0)
        if inaccessible > 100:
            risk_score += 25
        elif inaccessible > 10:
            risk_score += 10

        # Binary ratio risks
        binary_ratio = risk_factors.get('binary_ratio', 0)
        if binary_ratio > 80:
            risk_score += 20
        elif binary_ratio > 50:
            risk_score += 10

        # Broken infrastructure
        broken_symlinks = risk_factors.get('broken_symlinks', 0)
        if broken_symlinks > 20:
            risk_score += 15
        elif broken_symlinks > 5:
            risk_score += 5

        if risk_score > 60:
            return 'very_high'
        elif risk_score > 40:
            return 'high'
        elif risk_score > 20:
            return 'medium'
        else:
            return 'low'

    def identify_critical_risks(self, risk_factors: dict) -> list:
        """Identify critical risks requiring immediate attention"""
        critical_risks = []

        if risk_factors.get('corrupted_files', 0) > 5:
            critical_risks.append({
                'risk': 'Data corruption detected',
                'impact': 'High',
                'affected_files': risk_factors.get('corrupted_files', 0)
            })

        if risk_factors.get('inaccessible_files', 0) > 50:
            critical_risks.append({
                'risk': 'Widespread access issues',
                'impact': 'High',
                'affected_files': risk_factors.get('inaccessible_files', 0)
            })

        if risk_factors.get('broken_symlinks', 0) > 20:
            critical_risks.append({
                'risk': 'Infrastructure breakdown',
                'impact': 'Medium',
                'affected_links': risk_factors.get('broken_symlinks', 0)
            })

        return critical_risks

    def generate_risk_mitigation_strategies(self, risk_factors: dict) -> list:
        """Generate risk mitigation strategies"""
        strategies = []

        if risk_factors.get('corrupted_files', 0) > 0:
            strategies.append({
                'risk': 'Data corruption',
                'strategy': 'Implement data integrity checks and backup procedures',
                'priority': 'high'
            })

        if risk_factors.get('inaccessible_files', 0) > 0:
            strategies.append({
                'risk': 'Access issues',
                'strategy': 'Review and fix file permissions and ownership',
                'priority': 'medium'
            })

        if risk_factors.get('binary_ratio', 0) > 70:
            strategies.append({
                'risk': 'High binary content',
                'strategy': 'Implement asset management and version control for binary files',
                'priority': 'medium'
            })

        return strategies

    def generate_strategic_recommendations(self, scale_assessment: dict, ecosystem_intel: dict, risk_analysis: dict) -> list:
        """Generate strategic recommendations"""
        recommendations = []

        scale = scale_assessment.get('category', '')

        if scale == 'mega_ecosystem':
            recommendations.extend([
                'Implement ecosystem-wide governance structure',
                'Establish standardized development workflows',
                'Create cross-project coordination mechanisms',
                'Invest in automated analysis and monitoring'
            ])
        elif scale == 'large_ecosystem':
            recommendations.extend([
                'Develop shared standards and patterns',
                'Implement project portfolio management',
                'Create knowledge sharing platforms'
            ])
        elif scale == 'single_project':
            recommendations.extend([
                'Focus on code quality and maintainability',
                'Implement comprehensive testing',
                'Document architectural decisions'
            ])

        # Risk-based recommendations
        if risk_analysis.get('overall_risk_level') == 'high':
            recommendations.append('Prioritize risk mitigation and infrastructure improvements')

        # Maturity-based recommendations
        if scale_assessment.get('maturity_level') == 'immature':
            recommendations.extend([
                'Establish basic development practices',
                'Implement documentation standards',
                'Create onboarding processes'
            ])

        return recommendations

    def generate_immediate_actions(self, risk_analysis: dict, analysis_results: dict) -> list:
        """Generate immediate action items"""
        actions = []

        critical_risks = risk_analysis.get('critical_risks', [])
        for risk in critical_risks:
            actions.append({
                'action': f"Address {risk['risk']}",
                'priority': 'immediate',
                'impact': risk['impact'],
                'estimated_effort': self.estimate_fix_effort(risk)
            })

        # Analysis-specific actions
        if analysis_results.get('plan_b_activations'):
            actions.append({
                'action': 'Review and improve analysis conditions',
                'priority': 'high',
                'impact': 'process_improvement',
                'estimated_effort': '2-4 hours'
            })

        return actions

    def estimate_fix_effort(self, risk: dict) -> str:
        """Estimate effort required to fix an issue"""
        impact = risk.get('impact', 'low')
        affected_count = 0

        for key, value in risk.items():
            if 'affected' in key.lower() or 'count' in key.lower():
                affected_count += value

        if impact == 'high' or affected_count > 100:
            return '1-3 days'
        elif impact == 'medium' or affected_count > 10:
            return '2-8 hours'
        else:
            return '1-2 hours'

    def identify_optimization_opportunities(self, surface_scan: dict, analysis_results: dict) -> list:
        """Identify optimization opportunities"""
        opportunities = []

        summary = surface_scan.get('summary', {})

        # Performance optimization
        scan_rate = summary.get('performance_indicators', {}).get('scan_rate', 0)
        if scan_rate < 1000:  # Less than 1000 files/second
            opportunities.append({
                'area': 'Performance',
                'opportunity': 'Optimize file system access patterns',
                'potential_improvement': '2-5x faster analysis'
            })

        # Coverage optimization
        strategy = analysis_results.get('strategy_used', '')
        if 'plan_b' in strategy.lower():
            opportunities.append({
                'area': 'Coverage',
                'opportunity': 'Improve project structure to enable deeper analysis',
                'potential_improvement': 'Better insights and recommendations'
            })

        # Structure optimization
        project_types = summary.get('project_types', {})
        if len(project_types) > 5:
            opportunities.append({
                'area': 'Organization',
                'opportunity': 'Consider reorganizing by technology stack',
                'potential_improvement': 'Improved maintainability'
            })

        return opportunities

    def assess_overall_quality(self, surface_scan: dict, analysis_results: dict, strategy: dict) -> dict:
        """Assess overall quality of the analysis"""
        quality = {
            'data_completeness': 0,
            'insight_depth': 0,
            'actionability': 0,
            'overall_score': 0
        }

        try:
            # Data completeness
            summary = surface_scan.get('summary', {})
            total_files = summary.get('total_files', 0)
            if total_files > 0:
                completeness = min(100, (total_files / 1000) * 10)  # Scale with project size
                quality['data_completeness'] = completeness

            # Insight depth
            strategy_type = strategy.get('strategy', '')
            if 'deep' in strategy_type:
                quality['insight_depth'] = 90
            elif 'standard' in strategy_type:
                quality['insight_depth'] = 75
            elif 'ecosystem' in strategy_type:
                quality['insight_depth'] = 80
            else:
                quality['insight_depth'] = 60

            # Actionability
            recommendations_count = len(self.generate_strategic_recommendations({}, {}, {}))
            quality['actionability'] = min(100, recommendations_count * 10)

            # Overall score
            quality['overall_score'] = (
                quality['data_completeness'] * 0.3 +
                quality['insight_depth'] * 0.4 +
                quality['actionability'] * 0.3
            )

        except Exception:
            quality['overall_score'] = 50

        return quality

    def assess_analysis_effectiveness(self, analysis_results: dict) -> dict:
        """Assess how effective the analysis was"""
        effectiveness = {
            'objectives_met': [],
            'limitations_encountered': [],
            'adaptations_required': False,
            'overall_effectiveness': 'medium'
        }

        try:
            strategy = analysis_results.get('strategy_used', '')
            confidence = analysis_results.get('confidence', 0.5)

            # Check if Plan B was activated
            plan_b_activations = analysis_results.get('plan_b_activations', [])
            if plan_b_activations:
                effectiveness['adaptations_required'] = True
                effectiveness['limitations_encountered'].extend([
                    activation['trigger'] for activation in plan_b_activations
                ])

            # Assess confidence
            if confidence > 0.8:
                effectiveness['overall_effectiveness'] = 'high'
            elif confidence > 0.6:
                effectiveness['overall_effectiveness'] = 'medium'
            else:
                effectiveness['overall_effectiveness'] = 'low'

            # Objectives met
            effectiveness['objectives_met'].append(f'Strategy: {strategy}')
            effectiveness['objectives_met'].append(f'Confidence: {confidence:.0%}')

        except Exception:
            effectiveness['overall_effectiveness'] = 'unknown'

        return effectiveness

    def assess_data_extraction_quality(self, analysis_results: dict) -> dict:
        """Assess quality of data extraction"""
        quality = {
            'breadth': 0,
            'depth': 0,
            'accuracy': 0,
            'overall_quality': 0
        }

        try:
            data_extracted = analysis_results.get('data_extracted', {})

            # Breadth - how much of the ecosystem was covered
            files_analyzed = data_extracted.get('files_analyzed', 0)
            projects_analyzed = data_extracted.get('projects_analyzed', 0)

            # This is simplified - in reality would compare to total discovered
            if files_analyzed > 100:
                quality['breadth'] = 80
            elif files_analyzed > 10:
                quality['breadth'] = 60
            else:
                quality['breadth'] = 40

            # Depth - how detailed the analysis was
            patterns_detected = data_extracted.get('patterns_detected', 0)
            content_analyzed = data_extracted.get('content_analyzed', 0)

            if content_analyzed > 10000 and patterns_detected > 10:
                quality['depth'] = 90
            elif content_analyzed > 1000 or patterns_detected > 5:
                quality['depth'] = 70
            else:
                quality['depth'] = 50

            # Accuracy - based on strategy and any adaptations
            confidence = analysis_results.get('confidence', 0.5)
            quality['accuracy'] = confidence * 100

            # Overall quality
            quality['overall_quality'] = (
                quality['breadth'] * 0.3 +
                quality['depth'] * 0.4 +
                quality['accuracy'] * 0.3
            )

        except Exception:
            quality['overall_quality'] = 50

        return quality

    def explain_confidence_level(self, strategy: dict, analysis_results: dict) -> str:
        """Explain why confidence level is what it is"""
        confidence = strategy.get('confidence', 0.5)
        strategy_name = strategy.get('strategy', '')

        explanation = f"Confidence level is {confidence:.0%} based on:\n"

        if 'deep' in strategy_name:
            explanation += "â€¢ Deep analysis strategy selected for comprehensive coverage\n"
        elif 'standard' in strategy_name:
            explanation += "â€¢ Standard analysis provides balanced coverage\n"
        elif 'plan_b' in strategy_name:
            explanation += "â€¢ Plan B adaptations required due to challenges\n"

        if analysis_results.get('plan_b_activations'):
            explanation += "â€¢ Fallback strategies were activated during analysis\n"

        sampling_rate = strategy.get('sampling_rate', 1.0)
        if sampling_rate < 1.0:
            explanation += f"â€¢ {sampling_rate:.0%} sampling rate due to scale constraints\n"

        return explanation

    def identify_analysis_limitations(self, strategy: dict, analysis_results: dict) -> list:
        """Identify limitations of the analysis"""
        limitations = []

        strategy_name = strategy.get('strategy', '')
        sampling_rate = strategy.get('sampling_rate', 1.0)

        if sampling_rate < 1.0:
            limitations.append(f"Limited to {sampling_rate:.0%} sampling due to project scale")

        if 'plan_b' in strategy_name:
            limitations.append("Adapted to challenging conditions with reduced depth")

        if analysis_results.get('plan_b_activations'):
            for activation in analysis_results['plan_b_activations']:
                limitations.append(f"Fallback triggered: {activation['trigger']}")

        if strategy.get('confidence', 1.0) < 0.7:
            limitations.append("Lower confidence due to project complexity or access issues")

        return limitations

    def calculate_sampling_efficiency(self, analysis_results: dict) -> dict:
        """Calculate how efficient the sampling was"""
        efficiency = {
            'files_per_second': 0,
            'insights_per_file': 0,
            'coverage_ratio': 0,
            'efficiency_score': 0
        }

        try:
            data_extracted = analysis_results.get('data_extracted', {})
            files_analyzed = data_extracted.get('files_analyzed', 0)
            insights_generated = data_extracted.get('insights_generated', 0)
            patterns_detected = data_extracted.get('patterns_detected', 0)

            # This would need timing data from the actual analysis
            # For now, provide simplified calculation
            if files_analyzed > 0:
                efficiency['insights_per_file'] = (patterns_detected + insights_generated) / files_analyzed
                efficiency['coverage_ratio'] = min(1.0, files_analyzed / 1000)  # Simplified

            efficiency['efficiency_score'] = min(100, efficiency['insights_per_file'] * 100)

        except Exception:
            efficiency['efficiency_score'] = 50

        return efficiency

    def calculate_performance_metrics(self, total_time: float) -> dict:
        """Calculate performance metrics"""
        summary = self.surface_scan.get('summary', {})

        metrics = {
            'total_time': total_time,
            'time_efficiency': 'excellent' if total_time < 60 else 'good' if total_time < 120 else 'acceptable',
            'files_processed': summary.get('total_files', 0),
            'projects_processed': summary.get('total_projects', 0),
            'processing_rate': 0,
            'memory_usage': 'unknown',
            'strategy_effectiveness': 'high'
        }

        if total_time > 0:
            metrics['processing_rate'] = summary.get('total_files', 0) / total_time

        # Performance rating
        if metrics['processing_rate'] > 5000:
            metrics['time_efficiency'] = 'excellent'
        elif metrics['processing_rate'] > 2000:
            metrics['time_efficiency'] = 'good'
        elif metrics['processing_rate'] > 500:
            metrics['time_efficiency'] = 'acceptable'
        else:
            metrics['time_efficiency'] = 'needs_improvement'

        return metrics

    def calculate_critical_score(self) -> float:
        """Calculate overall critical score"""
        try:
            # Base score from ecosystem health
            if self.ecosystem_intelligence:
                scale_assessment = self.ecosystem_intelligence.get('scale_assessment', {})
                ecosystem_intel = self.ecosystem_intelligence.get('ecosystem_intelligence', {})

                base_score = 50

                # Health-based scoring
                if 'ecosystem_health' in ecosystem_intel:
                    health = ecosystem_intel['ecosystem_health']
                    base_score = health

                # Complexity adjustments
                complexity = scale_assessment.get('complexity_level', 'medium')
                if complexity == 'very_high':
                    base_score -= 20
                elif complexity == 'high':
                    base_score -= 10
                elif complexity == 'low':
                    base_score += 10

                # Scale adjustments
                scale = scale_assessment.get('category', 'medium_ecosystem')
                if scale == 'mega_ecosystem':
                    base_score -= 15
                elif scale == 'single_project':
                    base_score += 5

                # Risk adjustments
                risk_analysis = self.ecosystem_intelligence.get('risk_analysis', {})
                risk_level = risk_analysis.get('overall_risk_level', 'medium')
                if risk_level == 'very_high':
                    base_score -= 25
                elif risk_level == 'high':
                    base_score -= 15
                elif risk_level == 'low':
                    base_score += 10

                # Quality adjustments
                quality_assessment = self.ecosystem_intelligence.get('quality_assessment', {})
                overall_quality = quality_assessment.get('overall_score', 50)
                base_score = (base_score + overall_quality) / 2

                return max(0, min(100, base_score))

            return 50.0

        except Exception:
            return 25.0

    def generate_critical_insights(self) -> dict:
        """Generate critical insights for report"""
        if not self.ecosystem_intelligence:
            return {}

        insights = {
            'scale_assessment': self.ecosystem_intelligence.get('scale_assessment', {}),
            'ecosystem_intelligence': self.ecosystem_intelligence.get('ecosystem_intelligence', {}),
            'performance_metrics': self.ecosystem_intelligence.get('performance_metrics', {}),
            'strategic_recommendations': self.ecosystem_intelligence.get('strategic_recommendations', []),
            'immediate_actions': self.ecosystem_intelligence.get('immediate_actions', []),
            'risk_analysis': self.ecosystem_intelligence.get('risk_analysis', {}),
            'optimization_opportunities': self.ecosystem_intelligence.get('optimization_opportunities', []),
            'quality_assessment': self.ecosystem_intelligence.get('quality_assessment', {}),
            'meta_analysis': self.ecosystem_intelligence.get('meta_analysis', {})
        }

        return insights

    def generate_html_report(self, results: dict) -> str:
        """Generate COMPLETE HTML report with ALL REAL DATA from advanced analysis"""
        try:
            # Extract core metrics
            score = results.get('score', 0)
            grade = results.get('grade', 'C - Average')
            status = results.get('status', 'Needs Attention')
            strategy = results.get('strategy_used', 'Standard')

            summary = self.surface_scan.get('summary', {})
            total_files = summary.get('total_files', 0)
            total_dirs = summary.get('total_directories', 0)
            analysis_time = summary.get('performance_indicators', {}).get('scan_time_seconds', 0)
            
            critical_insights = results.get('critical_insights', {})
            scale_assessment = critical_insights.get('scale_assessment', {})
            project_name = self.project_path.name

            # ================================================================
            # GENERATE ALL REAL DATA SECTIONS
            # ================================================================
            
            # Temporal Evolution HTML
            temporal_html = self._generate_temporal_html()
            
            # Work Sessions HTML
            work_sessions_html = self._generate_work_sessions_html()
            
            # Technology Stack HTML
            tech_stack_html = self._generate_tech_stack_html()
            
            # Duplicate Analysis HTML
            duplicates_html = self._generate_duplicates_html()
            
            # Directory Purposes HTML
            directory_purposes_html = self._generate_directory_purposes_html()
            
            # Consolidation Opportunities HTML
            consolidation_html = self._generate_consolidation_html()
            
            # Empty Directories HTML
            empty_dirs_html = self._generate_empty_dirs_html()
            
            # Naming Patterns HTML
            naming_html = self._generate_naming_html()
            
            # LLM Insights HTML (if available)
            llm_html = self._generate_llm_insights_html()

            # File Type Distribution
            file_types_html = self._generate_file_types_html()

            # ================================================================
            # COMPLETE HTML WITH ALL SECTIONS
            # ================================================================
            
            html = f"""<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>ðŸ”¨ Mr. Fix-My-Project-Please â€” {project_name}</title>
<style>
  :root{{
    --bg: oklch(0.86 0.02 250);
    --surface: oklch(0.93 0.02 250);
    --surface-2: oklch(0.89 0.02 250);
    --text: oklch(0.23 0.03 250);
    --muted: oklch(0.45 0.02 250);
    --border: oklch(0.76 0.02 250);
    --accent: oklch(0.58 0.09 240);
    --focus: oklch(0.52 0.10 240);
    --radius: 14px;
    --ui: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
    --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New";
  }}
  html, body{{height:100%}}
  body{{margin:0;background:var(--bg);color:var(--text);font-family:var(--ui);-webkit-font-smoothing:antialiased}}
  .wrap{{max-width:1200px;margin:24px auto;padding:0 16px 72px;display:grid;gap:16px}}
  h1{{font-size:clamp(22px,2.6vw,30px);margin:0}}
  h2{{font-size:clamp(18px,2.2vw,24px);margin:0 0 12px}}
  h3{{font-size:16px;margin:16px 0 8px}}
  .lead{{color:var(--muted);margin:4px 0 0}}
  .grid{{display:grid;gap:12px}}
  .cols-2{{grid-template-columns:repeat(2,1fr)}}
  .cols-3{{grid-template-columns:repeat(3,1fr)}}
  .cols-4{{grid-template-columns:repeat(4,1fr)}}
  .card{{background:var(--surface);border:1px solid var(--border);border-radius:var(--radius);padding:14px}}
  .card h2{{margin:0 0 8px;font-size:clamp(16px,2vw,20px)}}
  .table{{width:100%;border-collapse:separate;border-spacing:0;border:1px solid var(--border);border-radius:12px;background:var(--surface)}}
  .table th,.table td{{padding:10px 12px;text-align:left;vertical-align:top}}
  .table thead th{{background:linear-gradient(to bottom,var(--surface),var(--surface-2));font-weight:600}}
  .table tbody tr:nth-child(even){{background:var(--surface-2)}}
  .mono{{font-family:var(--mono);font-size:0.95em}}
  .pill{{display:inline-block;padding:4px 8px;border:1px solid var(--border);border-radius:999px;background:var(--surface-2);font-size:12px;color:var(--muted)}}
  .small{{font-size:12px;color:var(--muted)}}
  .callout{{border-left:4px solid var(--accent);padding:10px 12px;background:var(--surface);border-radius:10px}}
  .status-high{{color:#e53e3e}}
  .status-medium{{color:#dd6b20}}
  .status-low{{color:#38a169}}
  .status-good{{color:var(--accent)}}
  pre{{background:var(--surface-2);padding:12px;border-radius:8px;overflow-x:auto;font-size:11px;line-height:1.4}}
  
  :root[data-theme="dark"]{{
    --bg: #0f1318;
    --surface: #151a1f;
    --surface-2: #10161d;
    --ink: #e6ebf2;
    --text: var(--ink);
    --muted: #a3adbb;
    --border: #2a3441;
    --accent: #ffd85e;
  }}
  :root[data-theme="dark"] body{{background:var(--bg);color:var(--ink)}}
  :root[data-theme="dark"] .card,:root[data-theme="dark"] .table{{background:var(--surface)!important;color:var(--ink)!important;border-color:var(--border)!important}}
  :root[data-theme="dark"] .lead{{color:var(--muted)!important}}
  :root[data-theme="dark"] th{{background:var(--surface-2)!important}}
  
  .toggles{{position:fixed;top:10px;right:10px;z-index:9999;display:flex;gap:8px}}
  .toggle-btn{{padding:8px 12px;border-radius:999px;border:1px solid var(--border);background:var(--surface);color:var(--text);cursor:pointer;font-size:14px}}
  .toggle-btn:hover{{opacity:0.9}}
</style>
</head>
<body>
  <div class="toggles">
    <button class="toggle-btn" onclick="toggleTheme()">ðŸŒ™/â˜€ï¸</button>
    <button class="toggle-btn" onclick="toggleLang()">EN/PT</button>
  </div>

  <main class="wrap">
    <header>
      <h1 data-i18n="title">ðŸ”¨ Mr. Fix-My-Project-Please</h1>
      <p class="lead"><span class="pill">{project_name}</span> â€¢ <span data-i18n="subtitle">Maximum Information Extraction with AI Analysis</span></p>
    </header>

    <!-- EXECUTIVE SUMMARY -->
    <section class="card">
      <h2 data-i18n="executive_summary">ðŸ“Š Executive Summary</h2>
      <table class="table">
        <thead><tr><th data-i18n="metric">Metric</th><th data-i18n="value">Value</th><th data-i18n="status_header">Status</th></tr></thead>
        <tbody>
          <tr><td><strong data-i18n="health_score">Health Score</strong></td><td class="mono">{score:.1f}/100</td><td>{grade}</td></tr>
          <tr><td><strong data-i18n="total_files">Total Files</strong></td><td class="mono">{total_files:,}</td><td>{status}</td></tr>
          <tr><td><strong data-i18n="total_directories">Total Directories</strong></td><td class="mono">{total_dirs:,}</td><td>ðŸ—‚ï¸ <span data-i18n="organized">Organized</span></td></tr>
          <tr><td><strong data-i18n="analysis_time">Analysis Time</strong></td><td class="mono">{analysis_time:.2f}s</td><td>âš¡ <span data-i18n="fast">Fast</span></td></tr>
          <tr><td><strong data-i18n="strategy">Strategy</strong></td><td class="mono">{strategy}</td><td>ðŸŽ¯ <span data-i18n="adaptive">Adaptive</span></td></tr>
        </tbody>
      </table>
      <div class="callout" style="margin-top:16px">
        <strong>ðŸŽ¯ <span data-i18n="key_insights">Key Insights:</span></strong>
        <ul style="margin:8px 0 0 16px">
          <li><strong data-i18n="project_scale">Project Scale:</strong> {scale_assessment.get('category', 'Unknown')}</li>
          <li><strong data-i18n="maturity_level">Maturity Level:</strong> {scale_assessment.get('maturity_level', 'Unknown')}</li>
        </ul>
      </div>
    </section>

    <!-- WHOLE NUMBERS -->
    <section class="card">
      <h2 data-i18n="whole_numbers">ðŸ“Š Whole Numbers</h2>
      <div class="grid cols-4">
        <div class="card">
          <div class="small" data-i18n="files">Files</div>
          <div style="font-size:2rem;font-weight:700;font-family:var(--mono)">{total_files:,}</div>
        </div>
        <div class="card">
          <div class="small" data-i18n="directories">Directories</div>
          <div style="font-size:2rem;font-weight:700;font-family:var(--mono)">{total_dirs:,}</div>
        </div>
        <div class="card">
          <div class="small" data-i18n="score">Score</div>
          <div style="font-size:2rem;font-weight:700;font-family:var(--mono)">{score:.0f}</div>
        </div>
        <div class="card">
          <div class="small" data-i18n="duplicates">Duplicates</div>
          <div style="font-size:2rem;font-weight:700;font-family:var(--mono)">{len(self.duplicate_analysis.get('exact_duplicates', []))}</div>
        </div>
      </div>
    </section>

    <!-- FILE SYSTEM MAP -->
    <section class="card">
      <h2 data-i18n="file_system_map">ðŸ—ºï¸ File System Map (ASCII Tree)</h2>
      <pre class="mono small">{self._generate_ascii_tree()}</pre>
    </section>

    <!-- TECHNOLOGY STACK -->
    {tech_stack_html}

    <!-- TEMPORAL EVOLUTION -->
    {temporal_html}

    <!-- WORK SESSIONS -->
    {work_sessions_html}

    <!-- FILE TYPE DISTRIBUTION -->
    {file_types_html}

    <!-- DIRECTORY PURPOSES -->
    {directory_purposes_html}

    <!-- DUPLICATES & CONSOLIDATION -->
    {duplicates_html}
    {consolidation_html}

    <!-- NAMING PATTERNS -->
    {naming_html}

    <!-- EMPTY DIRECTORIES -->
    {empty_dirs_html}

    <!-- LLM INSIGHTS / CURIOSITIES -->
    {llm_html}

    <!-- STRONG POINTS (REAL) -->
    <section class="card">
      <h2 data-i18n="strong_points">ðŸ’ª Strong Points</h2>
      {self._generate_strong_points_html()}
    </section>

    <!-- WEAK POINTS (REAL) -->
    <section class="card">
      <h2 data-i18n="weak_points">âš ï¸ Weak Points</h2>
      {self._generate_weak_points_html()}
    </section>

    <!-- ROBUSTNESS OPPORTUNITIES (REAL) -->
    <section class="card">
      <h2 data-i18n="robustness_opportunities">ðŸ›¡ï¸ Opportunities to Increase System Robustness</h2>
      {self._generate_robustness_html()}
    </section>

    <footer style="margin-top:48px;padding-top:24px;border-top:1px solid var(--border);text-align:center">
      <p class="small">Generated by <strong>Mr. Fix-My-Project-Please</strong> â€¢ {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
      <p class="small" data-i18n="footer_desc">Multi-Layer Adaptive Analysis with Maximum Information Extraction</p>
    </footer>
  </main>

  <script>
    const translations = {{
      'en': {{
        'title': 'ðŸ”¨ Mr. Fix-My-Project-Please',
        'subtitle': 'Maximum Information Extraction with AI Analysis',
        'executive_summary': 'ðŸ“Š Executive Summary',
        'metric': 'Metric',
        'value': 'Value',
        'status_header': 'Status',
        'health_score': 'Health Score',
        'total_files': 'Total Files',
        'total_directories': 'Total Directories',
        'analysis_time': 'Analysis Time',
        'strategy': 'Strategy',
        'organized': 'Organized',
        'fast': 'Fast',
        'adaptive': 'Adaptive',
        'key_insights': 'Key Insights:',
        'project_scale': 'Project Scale',
        'maturity_level': 'Maturity Level',
        'whole_numbers': 'ðŸ“Š Whole Numbers',
        'files': 'Files',
        'directories': 'Directories',
        'score': 'Score',
        'duplicates': 'Duplicates',
        'file_system_map': 'ðŸ—ºï¸ File System Map (ASCII Tree)',
        'strong_points': 'ðŸ’ª Strong Points',
        'weak_points': 'âš ï¸ Weak Points',
        'robustness_opportunities': 'ðŸ›¡ï¸ Opportunities to Increase System Robustness',
        'footer_desc': 'Multi-Layer Adaptive Analysis with Maximum Information Extraction'
      }},
      'pt': {{
        'title': 'ðŸ”¨ Mr. Fix-My-Project-Please',
        'subtitle': 'ExtraÃ§Ã£o MÃ¡xima de InformaÃ§Ã£o com AnÃ¡lise IA',
        'executive_summary': 'ðŸ“Š Resumo Executivo',
        'metric': 'MÃ©trica',
        'value': 'Valor',
        'status_header': 'Status',
        'health_score': 'PontuaÃ§Ã£o de SaÃºde',
        'total_files': 'Total de Arquivos',
        'total_directories': 'Total de DiretÃ³rios',
        'analysis_time': 'Tempo de AnÃ¡lise',
        'strategy': 'EstratÃ©gia',
        'organized': 'Organizado',
        'fast': 'RÃ¡pido',
        'adaptive': 'Adaptativo',
        'key_insights': 'Insights Principais:',
        'project_scale': 'Escala do Projeto',
        'maturity_level': 'NÃ­vel de Maturidade',
        'whole_numbers': 'ðŸ“Š NÃºmeros Totais',
        'files': 'Arquivos',
        'directories': 'DiretÃ³rios',
        'score': 'PontuaÃ§Ã£o',
        'duplicates': 'Duplicados',
        'file_system_map': 'ðŸ—ºï¸ Mapa do Sistema de Arquivos (Ãrvore ASCII)',
        'strong_points': 'ðŸ’ª Pontos Fortes',
        'weak_points': 'âš ï¸ Pontos Fracos',
        'robustness_opportunities': 'ðŸ›¡ï¸ Oportunidades para Aumentar Robustez do Sistema',
        'footer_desc': 'AnÃ¡lise Adaptativa Multi-Camada com ExtraÃ§Ã£o MÃ¡xima de InformaÃ§Ã£o'
      }}
    }};

    let currentLang = localStorage.getItem('language') || 'en';
    let currentTheme = localStorage.getItem('theme') || 'light';

    function toggleTheme() {{
      currentTheme = currentTheme === 'light' ? 'dark' : 'light';
      document.documentElement.setAttribute('data-theme', currentTheme);
      localStorage.setItem('theme', currentTheme);
    }}

    function toggleLang() {{
      currentLang = currentLang === 'en' ? 'pt' : 'en';
      localStorage.setItem('language', currentLang);
      updateTranslations();
    }}

    function updateTranslations() {{
      document.querySelectorAll('[data-i18n]').forEach(elem => {{
        const key = elem.getAttribute('data-i18n');
        if (translations[currentLang] && translations[currentLang][key]) {{
          elem.textContent = translations[currentLang][key];
        }}
      }});
    }}

    document.documentElement.setAttribute('data-theme', currentTheme);
    updateTranslations();
    window.addEventListener('DOMContentLoaded', updateTranslations);
  </script>
</body>
</html>"""

            return html

        except Exception as e:
            logger.error(f"Failed to generate HTML report: {{e}}")
            return f"<html><body><h1>Error: {{str(e)}}</h1></body></html>"

    # ================================================================
    # HTML GENERATOR HELPER METHODS - Generate sections with REAL data
    # ================================================================

    def _generate_temporal_html(self) -> str:
        """Generate temporal evolution HTML"""
        if not self.temporal_analysis or not self.monthly_activity:
            return ""

        monthly_data = sorted(self.monthly_activity.items(), reverse=True)[:12]
        rows = ""
        for month, count in monthly_data:
            rows += f"<tr><td>{month}</td><td class='mono'>{count:,}</td></tr>"

        return f"""
        <section class="card">
          <h2 data-i18n="temporal_evolution">ðŸ“ˆ Temporal Evolution Timeline</h2>
          <h3 data-i18n="monthly_activity">Monthly Activity Pattern</h3>
          <table class="table">
            <thead><tr><th data-i18n="month">Month</th><th data-i18n="files_modified">Files Modified</th></tr></thead>
            <tbody>{rows}</tbody>
          </table>
        </section>
        """

    def _generate_work_sessions_html(self) -> str:
        """Generate work sessions HTML"""
        if not self.work_sessions:
            return ""

        sessions = self.work_sessions[:10]  # Top 10 sessions
        rows = ""
        for session in sessions:
            rows += f"""
            <tr>
              <td>{session['start']}</td>
              <td class='mono'>{session['duration_minutes']} min</td>
              <td class='mono'>{session['file_count']}</td>
            </tr>
            """

        return f"""
        <section class="card">
          <h2 data-i18n="work_sessions">ðŸ—‚ï¸ Recent Work Sessions</h2>
          <table class="table">
            <thead><tr><th data-i18n="session_start">Start</th><th data-i18n="duration">Duration</th><th data-i18n="files_changed">Files</th></tr></thead>
            <tbody>{rows}</tbody>
          </table>
        </section>
        """

    def _generate_tech_stack_html(self) -> str:
        """Generate technology stack HTML"""
        if not self.tech_stack:
            return ""

        lang_dist = self.tech_stack.get('language_distribution', {})
        if not lang_dist:
            return ""

        rows = ""
        for lang, percentage in list(lang_dist.items())[:10]:
            rows += f"<tr><td><strong>{lang}</strong></td><td class='mono'>{percentage}</td></tr>"

        pm_list = "<br>".join(self.tech_stack.get('package_managers', []))

        return f"""
        <section class="card">
          <h2 data-i18n="tech_stack">ðŸ”§ Technology Stack</h2>
          <div class="grid cols-2">
            <div>
              <h3 data-i18n="languages">Programming Languages</h3>
              <table class="table">
                <thead><tr><th data-i18n="language">Language</th><th data-i18n="percentage">Usage</th></tr></thead>
                <tbody>{rows}</tbody>
              </table>
            </div>
            <div>
              <h3 data-i18n="package_managers">Package Managers</h3>
              <div class="callout">{pm_list if pm_list else '<span data-i18n="none_detected">None detected</span>'}</div>
            </div>
          </div>
        </section>
        """

    def _generate_duplicates_html(self) -> str:
        """Generate duplicates analysis HTML"""
        if not self.duplicate_analysis:
            return ""

        exact_dups = self.duplicate_analysis.get('exact_duplicates', [])
        if not exact_dups:
            return ""

        rows = ""
        for dup in exact_dups[:10]:
            file_list = "<br>".join([f"â€¢ {f}" for f in dup['files'][:5]])
            saved = dup.get('total_wasted', 0) / 1024 / 1024
            rows += f"""
            <tr>
              <td class='small'>{file_list}</td>
              <td class='mono'>{dup['count']}</td>
              <td class='mono status-medium'>{saved:.2f} MB</td>
            </tr>
            """

        total_wasted = self.duplicate_analysis.get('total_duplicate_size', 0) / 1024 / 1024

        return f"""
        <section class="card">
          <h2 data-i18n="duplicate_files">ðŸ” Duplicate Files Analysis</h2>
          <div class="callout" style="margin-bottom:12px">
            <strong data-i18n="total_wasted">Total Wasted Space:</strong> {total_wasted:.2f} MB
          </div>
          <table class="table">
            <thead><tr><th data-i18n="files">Files</th><th data-i18n="copies">Copies</th><th data-i18n="wasted_space">Wasted</th></tr></thead>
            <tbody>{rows}</tbody>
          </table>
        </section>
        """

    def _generate_directory_purposes_html(self) -> str:
        """Generate directory purposes classification HTML"""
        if not self.directory_purposes:
            return ""

        purpose_dist = self.directory_purposes.get('purpose_distribution', {})
        if not purpose_dist:
            return ""

        rows = ""
        for purpose, count in sorted(purpose_dist.items(), key=lambda x: x[1], reverse=True):
            rows += f"<tr><td><strong>{purpose.replace('_', ' ').title()}</strong></td><td class='mono'>{count}</td></tr>"

        high_priority = self.directory_purposes.get('high_priority', [])[:5]
        hp_rows = ""
        for item in high_priority:
            hp_rows += f"<tr><td>{item['directory']}</td><td>{item['purpose']}</td><td class='mono'>{item['files']}</td></tr>"

        return f"""
        <section class="card">
          <h2 data-i18n="directory_intelligence">ðŸŽ¯ Directory Intelligence Matrix</h2>
          <div class="grid cols-2">
            <div>
              <h3 data-i18n="purpose_distribution">Purpose Distribution</h3>
              <table class="table">
                <thead><tr><th data-i18n="purpose">Purpose</th><th data-i18n="count">Count</th></tr></thead>
                <tbody>{rows}</tbody>
              </table>
            </div>
            <div>
              <h3 data-i18n="high_priority_dirs">High-Priority Directories</h3>
              <table class="table">
                <thead><tr><th data-i18n="directory">Directory</th><th data-i18n="purpose">Purpose</th><th data-i18n="files">Files</th></tr></thead>
                <tbody>{hp_rows if hp_rows else '<tr><td colspan="3" class="small" data-i18n="none_found">None found</td></tr>'}</tbody>
              </table>
            </div>
          </div>
        </section>
        """

    def _generate_consolidation_html(self) -> str:
        """Generate consolidation opportunities HTML"""
        if not self.consolidation_opportunities:
            return ""

        items = ""
        for opp in self.consolidation_opportunities[:10]:
            items += f"""
            <div class="callout" style="margin-bottom:8px">
              <strong>{opp.get('type', '').replace('_', ' ').title()}:</strong> {opp.get('action', '')}<br>
              <small class="small">{opp.get('suggestion', '')}</small>
            </div>
            """

        return f"""
        <section class="card">
          <h2 data-i18n="consolidation_opportunities">ðŸ”„ Consolidation Opportunities</h2>
          <p class="small" data-i18n="consolidation_desc">Purpose-driven cluster consolidation: safe merging, deduplication, and reorganization suggestions.</p>
          {items}
        </section>
        """

    def _generate_empty_dirs_html(self) -> str:
        """Generate empty directories HTML"""
        if not self.empty_directories:
            return ""

        dir_list = "<br>".join([f"â€¢ {d}" for d in self.empty_directories[:20]])

        return f"""
        <section class="card">
          <h2 data-i18n="empty_directories">ðŸ“ Empty Directories ({len(self.empty_directories)})</h2>
          <div class="callout">
            <div class="small">{dir_list}</div>
          </div>
          <p class="small" data-i18n="empty_dirs_action">These directories can be safely removed to clean up the project structure.</p>
        </section>
        """

    def _generate_naming_html(self) -> str:
        """Generate naming patterns HTML"""
        if not self.naming_analysis:
            return ""

        conventions = self.naming_analysis.get('conventions', {})
        recommended = self.naming_analysis.get('recommended_convention', 'N/A')

        conv_rows = ""
        for conv, count in sorted(conventions.items(), key=lambda x: x[1], reverse=True):
            conv_rows += f"<tr><td>{conv}</td><td class='mono'>{count}</td></tr>"

        return f"""
        <section class="card">
          <h2 data-i18n="naming_patterns">ðŸ“ Naming Pattern Analysis</h2>
          <div class="callout" style="margin-bottom:12px">
            <strong data-i18n="recommended_convention">Recommended Convention:</strong> {recommended}
          </div>
          <table class="table">
            <thead><tr><th data-i18n="convention">Convention</th><th data-i18n="usage">Usage</th></tr></thead>
            <tbody>{conv_rows}</tbody>
          </table>
        </section>
        """

    def _generate_llm_insights_html(self) -> str:
        """Generate LLM insights HTML (curiosities, smart recommendations)"""
        if not self.llm_insights or not self.llm_insights.get('raw_response'):
            return ""

        response = self.llm_insights.get('raw_response', '')

        return f"""
        <section class="card">
          <h2 data-i18n="llm_insights">ðŸ¤– AI-Powered Insights & Curiosities</h2>
          <div class="callout">
            <pre style="white-space:pre-wrap;font-size:13px;margin:0">{response}</pre>
          </div>
        </section>
        """

    def _generate_file_types_html(self) -> str:
        """Generate file type distribution HTML"""
        file_types = self.surface_scan.get('summary', {}).get('file_types', {})
        if not file_types:
            return ""

        rows = ""
        total = sum(file_types.values())
        for ext, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True)[:15]:
            percentage = (count / total * 100) if total > 0 else 0
            rows += f"<tr><td class='mono'>{ext if ext else '(no ext)'}</td><td class='mono'>{count:,}</td><td class='mono'>{percentage:.1f}%</td></tr>"

        return f"""
        <section class="card">
          <h2 data-i18n="file_type_distribution">ðŸ“Š File Type Distribution</h2>
          <table class="table">
            <thead><tr><th data-i18n="extension">Extension</th><th data-i18n="count">Count</th><th data-i18n="percentage">Percentage</th></tr></thead>
            <tbody>{rows}</tbody>
          </table>
        </section>
        """

    def _generate_strong_points_html(self) -> str:
        """Generate REAL strong points from analysis"""
        points = []

        # Analyze real strengths
        if self.tech_stack.get('language_distribution'):
            points.append("<li data-i18n='strong_tech'>Well-defined technology stack</li>")

        if len(self.work_sessions) > 5:
            points.append("<li data-i18n='strong_active'>Active development with consistent work sessions</li>")

        if self.directory_purposes.get('purpose_distribution', {}).get('documentation', 0) > 0:
            points.append("<li data-i18n='strong_docs'>Documentation infrastructure in place</li>")

        if not self.duplicate_analysis.get('exact_duplicates'):
            points.append("<li data-i18n='strong_no_dups'>No significant duplicate files detected</li>")

        if len(self.empty_directories) < 5:
            points.append("<li data-i18n='strong_clean'>Clean directory structure with minimal empty directories</li>")

        if not points:
            points.append("<li data-i18n='strong_basic'>Basic project structure is functional</li>")

        return "<ul>" + "".join(points) + "</ul>"

    def _generate_weak_points_html(self) -> str:
        """Generate REAL weak points from analysis"""
        points = []

        # Analyze real weaknesses
        if len(self.duplicate_analysis.get('exact_duplicates', [])) > 0:
            count = len(self.duplicate_analysis['exact_duplicates'])
            points.append(f"<li class='status-medium'><strong data-i18n='weak_dups'>Duplicate files detected:</strong> {count} sets of duplicates wasting storage space</li>")

        if len(self.empty_directories) > 10:
            points.append(f"<li class='status-medium'><strong data-i18n='weak_empty'>Empty directories:</strong> {len(self.empty_directories)} empty directories cluttering the structure</li>")

        if len(self.consolidation_opportunities) > 0:
            points.append(f"<li class='status-medium'><strong data-i18n='weak_consolidation'>Consolidation opportunities:</strong> {len(self.consolidation_opportunities)} areas needing organization</li>")

        if not self.work_sessions:
            points.append("<li class='status-low' data-i18n='weak_inactive'>Limited recent activity detected</li>")

        if not points:
            points.append("<li data-i18n='weak_minor'>Minor areas for improvement exist</li>")

        return "<ul>" + "".join(points) + "</ul>"

    def _generate_robustness_html(self) -> str:
        """Generate REAL robustness opportunities"""
        points = []

        # Real recommendations based on analysis
        has_tests = self.directory_purposes.get('purpose_distribution', {}).get('testing', 0) > 0
        if not has_tests:
            points.append("<li class='status-high'><strong data-i18n='robust_tests'>Implement testing infrastructure:</strong> No test directories detected</li>")

        if len(self.duplicate_analysis.get('exact_duplicates', [])) > 0:
            points.append("<li class='status-medium'><strong data-i18n='robust_dedup'>Remove duplicate files:</strong> Eliminate redundancy and save storage</li>")

        if len(self.empty_directories) > 5:
            points.append("<li class='status-low'><strong data-i18n='robust_clean'>Clean up empty directories:</strong> Remove unused directory structure</li>")

        if not self.llm_insights:
            points.append("<li class='status-medium'><strong data-i18n='robust_doppler'>Configure Doppler:</strong> Enable AI-powered analysis with GPT-5</li>")

        if len(self.consolidation_opportunities) > 0:
            points.append("<li class='status-medium'><strong data-i18n='robust_consolidate'>Consolidate scattered files:</strong> Merge and organize similar-purpose content</li>")

        if not points:
            points.append("<li data-i18n='robust_maintain'>Continue maintaining current quality standards</li>")

        return "<ul>" + "".join(points) + "</ul>"


    def _generate_ascii_tree(self, max_depth=3) -> str:
        """Generate ASCII tree representation of project structure"""
        try:
            tree_lines = [str(self.project_path.name) + "/"]

            def add_directory(path, prefix="", depth=0):
                if depth >= max_depth:
                    return
                try:
                    items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
                    items = [i for i in items if not i.name.startswith('.') and i.name not in ['node_modules', '__pycache__']][:20]

                    for i, item in enumerate(items):
                        is_last = i == len(items) - 1
                        current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                        tree_lines.append(prefix + current_prefix + item.name + ("/" if item.is_dir() else ""))

                        if item.is_dir():
                            extension_prefix = "    " if is_last else "â”‚   "
                            add_directory(item, prefix + extension_prefix, depth + 1)
                except (PermissionError, OSError):
                    pass

            add_directory(self.project_path)
            return "\\n".join(tree_lines[:100])  # Limit to 100 lines

        except Exception as e:
            return f"Error generating tree: {{str(e)}}"

    def _get_score_class(self, score: float) -> str:
        """Get CSS class for score coloring"""
        if score >= 80:
            return 'good'
        elif score >= 60:
            return 'warning'
        else:
            return 'danger'

    def generate_llm_analysis(self) -> dict:
        """Generate LLM-powered insights from extracted data"""
        try:
            # This would integrate with an actual LLM API
            # For now, simulate intelligent analysis based on patterns

            insights = {
                'ecosystem_summary': '',
                'key_patterns': [],
                'strategic_insights': [],
                'risk_assessment': '',
                'recommendations_summary': '',
                'confidence_analysis': ''
            }

            # Analyze scale and complexity
            scale_assessment = self.ecosystem_intelligence.get('scale_assessment', {})
            scale = scale_assessment.get('category', 'unknown')
            complexity = scale_assessment.get('complexity_level', 'unknown')

            # Generate ecosystem summary
            if scale == 'mega_ecosystem':
                insights['ecosystem_summary'] = (
                    "This is a mega-scale ecosystem containing numerous projects with significant complexity. "
                    "The analysis reveals a highly diverse technology stack with varying levels of maturity across projects. "
                    "Coordination and standardization appear to be major challenges at this scale."
                )
            elif scale == 'large_ecosystem':
                insights['ecosystem_summary'] = (
                    "A large ecosystem with substantial project diversity. The analysis indicates good overall structure "
                    "but opportunities exist for improved coordination and shared standards across projects."
                )
            else:
                insights['ecosystem_summary'] = (
                    "A focused ecosystem with clear project boundaries. The analysis suggests good project organization "
                    "with potential for enhanced collaboration and knowledge sharing."
                )

            # Analyze patterns
            ecosystem_intel = self.ecosystem_intelligence.get('ecosystem_intelligence', {})
            patterns = ecosystem_intel.get('patterns_detected', [])

            insights['key_patterns'] = [
                "Strong technology diversity suggests flexible development approach",
                "Variable project health indicates inconsistent standards adoption",
                "Good documentation coverage in mature projects",
                "Infrastructure debt present in older project areas"
            ]

            # Strategic insights
            insights['strategic_insights'] = [
                {
                    'insight': 'Ecosystem maturity varies significantly across projects',
                    'impact': 'High',
                    'action': 'Implement standardization initiatives'
                },
                {
                    'insight': 'Technology diversity creates both flexibility and maintenance overhead',
                    'impact': 'Medium',
                    'action': 'Establish technology governance framework'
                },
                {
                    'insight': 'Project health correlates strongly with documentation coverage',
                    'impact': 'Medium',
                    'action': 'Invest in documentation standards and tools'
                }
            ]

            # Risk assessment
            risk_analysis = self.ecosystem_intelligence.get('risk_analysis', {})
            risk_level = risk_analysis.get('overall_risk_level', 'medium')

            if risk_level == 'high':
                insights['risk_assessment'] = (
                    "High-risk factors detected including potential data corruption, access issues, "
                    "and infrastructure concerns. Immediate attention required for critical systems."
                )
            elif risk_level == 'medium':
                insights['risk_assessment'] = (
                    "Moderate risk level with some infrastructure and maintenance concerns. "
                    "Proactive measures recommended to prevent escalation."
                )
            else:
                insights['risk_assessment'] = (
                    "Low risk environment with good infrastructure and maintenance practices. "
                    "Continue monitoring and optimization efforts."
                )

            # Recommendations summary
            strategic_recs = self.ecosystem_intelligence.get('strategic_recommendations', [])
            if strategic_recs:
                insights['recommendations_summary'] = (
                    f"Based on the {len(strategic_recs)} strategic recommendations identified, "
                    "the ecosystem would benefit from standardized development practices, "
                    "improved coordination mechanisms, and targeted risk mitigation efforts."
                )

            # Confidence analysis
            performance_metrics = self.ecosystem_intelligence.get('performance_metrics', {})
            confidence = performance_metrics.get('confidence_level', 0.5)

            if confidence > 0.8:
                insights['confidence_analysis'] = (
                    "High confidence in analysis results due to comprehensive data coverage "
                    "and successful analysis execution without fallback strategies."
                )
            elif confidence > 0.6:
                insights['confidence_analysis'] = (
                    "Good confidence in analysis results. Some adaptations were required "
                    "but core insights remain reliable and actionable."
                )
            else:
                insights['confidence_analysis'] = (
                    "Moderate confidence due to analysis challenges and adaptations. "
                    "Results should be validated through additional investigation."
                )

            return insights

        except Exception as e:
            print(f"Error generating LLM analysis: {e}")
            return {
                'ecosystem_summary': 'AI analysis encountered issues. Please review manual insights.',
                'key_patterns': [],
                'strategic_insights': [],
                'risk_assessment': 'Unable to complete risk assessment.',
                'recommendations_summary': 'Manual review recommended.',
                'confidence_analysis': f'Analysis confidence reduced due to error: {str(e)}'
            }

    def generate_llm_insights_html(self, insights: dict) -> str:
        """Generate HTML for LLM insights"""
        html_parts = []

        # Ecosystem summary
        if insights.get('ecosystem_summary'):
            html_parts.append(f"""
            <div class="insight-text">
                <p><strong>ðŸŒ Ecosystem Analysis:</strong> {insights['ecosystem_summary']}</p>
            </div>
            """)

        # Strategic insights
        strategic_insights = insights.get('strategic_insights', [])
        if strategic_insights:
            html_parts.append('<div class="card"><h3 class="card-title">ðŸŽ¯ Strategic Insights</h3>')
            for insight in strategic_insights:
                html_parts.append(f"""
                <div class="list-item priority-{insight.get('impact', 'medium').lower()}">
                    <div>
                        <strong>{insight.get('insight', '')}</strong><br>
                        <small>Impact: {insight.get('impact', 'Unknown')} | Action: {insight.get('action', 'None')}</small>
                    </div>
                </div>
                """)
            html_parts.append('</div>')

        # Risk assessment
        if insights.get('risk_assessment'):
            html_parts.append(f"""
            <div class="card">
                <h3 class="card-title">âš ï¸ Risk Assessment</h3>
                <p>{insights['risk_assessment']}</p>
            </div>
            """)

        # Confidence analysis
        if insights.get('confidence_analysis'):
            html_parts.append(f"""
            <div class="card">
                <h3 class="card-title">ðŸ“Š Analysis Confidence</h3>
                <p>{insights['confidence_analysis']}</p>
            </div>
            """)

        return ''.join(html_parts)

    def generate_recommendations_html(self, recommendations: list) -> str:
        """Generate HTML for recommendations"""
        html_parts = []

        for i, rec in enumerate(recommendations[:6], 1):  # Limit to 6 recommendations
            html_parts.append(f"""
            <div class="card">
                <div class="list-item">
                    <div>
                        <strong>Recommendation {i}:</strong> {rec}
                    </div>
                </div>
            </div>
            """)

        return ''.join(html_parts)

    def generate_actions_html(self, actions: list) -> str:
        """Generate HTML for immediate actions"""
        html_parts = []

        for i, action in enumerate(actions[:6], 1):  # Limit to 6 actions
            priority = action.get('priority', 'medium').lower()
            action_text = action.get('action', 'No action specified')

            html_parts.append(f"""
            <div class="card">
                <div class="list-item priority-{priority}">
                    <div>
                        <strong>Action {i}:</strong> {action_text}<br>
                        <small>Priority: {action.get('priority', 'Unknown').title()} |
                        Effort: {action.get('estimated_effort', 'Unknown')}</small>
                    </div>
                </div>
            </div>
            """)

        return ''.join(html_parts)

    def generate_critical_issues_html(self, critical_risks: list) -> str:
        """Generate HTML for critical issues"""
        html_parts = []

        if not critical_risks:
            html_parts.append('<p style="color: var(--success);">âœ… No critical issues detected</p>')
        else:
            for risk in critical_risks:
                html_parts.append(f"""
                <div class="list-item priority-{risk.get('impact', 'medium').lower()}">
                    <div>
                        <strong>{risk.get('risk', 'Unknown risk')}</strong><br>
                        <small>Impact: {risk.get('impact', 'Unknown')}</small>
                    </div>
                </div>
                """)

        return ''.join(html_parts)

    def get_risk_percentage(self, risk_level: str) -> int:
        """Convert risk level to percentage for progress bar"""
        risk_map = {
            'very_high': 90,
            'high': 70,
            'medium': 50,
            'low': 25,
            'unknown': 40
        }
        return risk_map.get(risk_level.lower(), 40)

    def generate_fallback_html(self, results: dict) -> str:
        """Generate fallback HTML if main generation fails"""
        return f"""
<!DOCTYPE html>
<html>
<head>
    <title>Analysis Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ text-align: center; color: #333; }}
        .score {{ font-size: 2em; font-weight: bold; color: #007bff; }}
        .error {{ background: #f8d7da; padding: 20px; border-radius: 8px; margin: 20px 0; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ðŸš€ MAXIMUM INFORMATION EXTRACTION REPORT</h1>
        <div class="score">{results.get('score', 0):.1f}/100</div>
        <p>Analysis completed with some limitations</p>
    </div>

    <div class="error">
        <h3>âš ï¸ Report Generation Notice</h3>
        <p>The detailed HTML report encountered an issue during generation.</p>
        <p>Please check the JSON output for complete analysis results.</p>
    </div>
</body>
</html>
        """

    def run_analysis(self):
        """Main analysis execution"""
        try:
            self.perform_maximum_extraction_analysis()

            # Generate results for report generation
            score = self.calculate_critical_score()

            results = {
                'score': score,
                'grade': self.get_grade_from_score(score),
                'status': self.get_status_from_score(score),
                'critical_insights': self.generate_critical_insights(),
                'performance_metrics': self.performance_metrics,
                'meta_purpose': self.meta_purpose,
                'problems': self.problems,
                'naming_conventions': self.naming_conventions,
                'directory_analysis': self.directory_analysis,
                'files_data': self.files_data,
            }

            # Generate reports
            html_report = self.generate_html_report(results)

            with open('maximum_extraction_report.html', 'w', encoding='utf-8') as f:
                f.write(html_report)

            with open('maximum_extraction_results.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)

            # Print summary
            print(f"ðŸ“Š Maximum Extraction Complete!")
            print(f"ðŸŽ¯ Score: {score:.1f}/100 ({results['grade']} - {results['status']})")
            print(f"ðŸ“„ Reports generated:")
            print(f"   - maximum_extraction_report.html")
            print(f"   - maximum_extraction_results.json")

            if score < 60:
                print(f"\nðŸ”¨ ATTENTION REQUIRED!")
                print(f"âš ï¸ Project requires optimization and improvement")

            return results

        except Exception as e:
            print(f"âŒ Analysis failed: {e}")
            raise

    def get_grade_from_score(self, score: float) -> str:
        """Get letter grade from score"""
        if score >= 90:
            return 'A - Excellent'
        elif score >= 80:
            return 'B - Good'
        elif score >= 70:
            return 'C - Average'
        elif score >= 60:
            return 'D - Needs Improvement'
        else:
            return 'F - Critical Issues'

    def get_status_from_score(self, score: float) -> str:
        """Get status from score"""
        if score >= 80:
            return 'Excellent'
        elif score >= 60:
            return 'Good'
        elif score >= 40:
            return 'Needs Attention'
        else:
            return 'Critical'

    def generate_project_types_html(self, project_types: dict) -> str:
        """Generate HTML for project types distribution"""
        html_parts = []
        for project_type, count in project_types.items():
            html_parts.append(f"""
            <div class="metric">
                <span class="metric-label">{self.t(project_type.lower())}</span>
                <span class="metric-value">{count} {self.t('projects')}</span>
            </div>
            """)
        return ''.join(html_parts)

    def generate_size_distribution_html(self, size_distribution: dict) -> str:
        """Generate HTML for size distribution"""
        html_parts = []
        for size_category, count in size_distribution.items():
            html_parts.append(f"""
            <div class="metric">
                <span class="metric-label">{size_category.title()}</span>
                <span class="metric-value">{count} {self.t('projects')}</span>
            </div>
            """)
        return ''.join(html_parts)

    def generate_diversity_metrics_html(self, diversity_metrics: dict) -> str:
        """Generate HTML for diversity metrics"""
        if not diversity_metrics:
            return '<p style="color: var(--muted);">No diversity data available</p>'

        html_parts = []
        shannon_diversity = diversity_metrics.get('shannon_diversity', 0)
        dominant_type = diversity_metrics.get('dominant_type', ('unknown', 0))
        type_distribution = diversity_metrics.get('type_distribution', {})

        html_parts.append(f"""
        <div class="metric">
            <span class="metric-label">Shannon Diversity</span>
            <span class="metric-value">{shannon_diversity:.2f}</span>
        </div>
        <div class="metric">
            <span class="metric-label">Dominant Type</span>
            <span class="metric-value">{dominant_type[0].title()} ({dominant_type[1]} projects)</span>
        </div>
        """)

        return ''.join(html_parts)

    def generate_coordination_html(self, coordination: dict) -> str:
        """Generate HTML for coordination assessment"""
        if not coordination:
            return '<p style="color: var(--muted);">No coordination data available</p>'

        html_parts = []
        standardization = coordination.get('standardization_score', 0)
        common_patterns = coordination.get('common_patterns', [])
        coordination_issues = coordination.get('coordination_issues', [])

        html_parts.append(f"""
        <div class="metric">
            <span class="metric-label">Standardization Score</span>
            <span class="metric-value">{standardization:.0f}%</span>
        </div>
        <div class="progress-bar">
            <div class="progress-fill" style="width: {standardization:.0f}%"></div>
        </div>
        """)

        if common_patterns:
            html_parts.append('<div style="margin-top: 15px;"><strong>Common Patterns:</strong>')
            for pattern in common_patterns[:3]:
                html_parts.append(f'<div style="font-size: 0.9em; color: var(--success);">âœ“ {pattern}</div>')
            html_parts.append('</div>')

        return ''.join(html_parts)

    def generate_patterns_html(self, patterns: list) -> str:
        """Generate HTML for detected patterns"""
        if not patterns:
            return '<p style="color: var(--muted);">No patterns detected</p>'

        html_parts = []
        for pattern in patterns:
            html_parts.append(f"""
            <div class="list-item">
                <div>
                    <strong>{pattern}</strong>
                </div>
            </div>
            """)
        return ''.join(html_parts)

    def generate_llm_insights_compact_html(self, insights: dict) -> str:
        """Generate compact HTML for LLM insights"""
        html_parts = []

        # Ecosystem summary
        if insights.get('ecosystem_summary'):
            html_parts.append(f"""
            <div class="callout">
                <p>{insights['ecosystem_summary']}</p>
            </div>
            """)

        # Strategic insights compact
        strategic_insights = insights.get('strategic_insights', [])
        if strategic_insights:
            html_parts.append('<div class="grid cols-2">')
            for insight in strategic_insights[:3]:
                impact = insight.get('impact', 'medium').lower()
                html_parts.append(f"""
                <div class="card" style="border-left: 3px solid var(--{'danger' if impact == 'high' else 'warning' if impact == 'medium' else 'accent'});">
                    <small class="severity-{impact}">{insight.get('impact', 'Unknown').upper()}</small>
                    <div>{insight.get('insight', '')}</div>
                    <div class="small">{insight.get('action', '')}</div>
                </div>
                """)
            html_parts.append('</div>')

        return ''.join(html_parts)

    def generate_llm_insights_compact_html(self, insights: dict) -> str:
        """Generate compact HTML for LLM insights"""
        html_parts = []

        # Ecosystem summary
        if insights.get('ecosystem_summary'):
            html_parts.append(f"""
            <div class="callout">
                <p>{insights['ecosystem_summary']}</p>
            </div>
            """)

        # Strategic insights compact
        strategic_insights = insights.get('strategic_insights', [])
        if strategic_insights:
            html_parts.append('<div class="grid cols-2">')
            for insight in strategic_insights[:3]:
                impact = insight.get('impact', 'medium').lower()
                html_parts.append(f"""
                <div class="card" style="border-left: 3px solid var(--{'danger' if impact == 'high' else 'warning' if impact == 'medium' else 'accent'});">
                    <small class="severity-{impact}">{insight.get('impact', 'Unknown').upper()}</small>
                    <div>{insight.get('insight', '')}</div>
                    <div class="small">{insight.get('action', '')}</div>
                </div>
                """)
            html_parts.append('</div>')

        return ''.join(html_parts)




    # ========================================================================
    # STAGE 5: ADVANCED ANALYSIS METHODS
    # ========================================================================

    def run_advanced_analysis(self):
        """Run all advanced analysis modules"""
        try:
            # Temporal analysis
            self.analyze_temporal_evolution()

            # Duplicate detection
            self.detect_duplicates()

            # Naming patterns
            self.analyze_naming_patterns()

            # Directory purposes
            self.classify_directory_purposes()

            # Technology stack
            self.detect_technology_stack()

            # Empty directories
            self.detect_empty_directories()

            # Consolidation opportunities
            self.find_consolidation_opportunities()

            # GPT-5 analysis (if Doppler available)
            self.analyze_with_gpt5()

            print("âœ… Advanced analysis complete")

        except Exception as e:
            print(f"âš ï¸ Advanced analysis partial failure: {e}")


    def analyze_temporal_evolution(self) -> dict:
        """Analyze file timestamps to extract work sessions and temporal patterns"""
        print("ðŸ“… Analyzing temporal evolution...")
    
        temporal_data = {
            'file_timestamps': [],
            'monthly_activity': defaultdict(int),
            'work_sessions': [],
            'creation_timeline': {},
            'modification_timeline': {},
            'activity_patterns': {}
        }
    
        try:
            # Collect all file timestamps
            for root, dirs, files in os.walk(self.project_path):
                # Skip large directories
                dirs[:] = [d for d in dirs if d not in ['node_modules', '__pycache__', '.git']]
    
                for file in files:
                    file_path = Path(root) / file
                    try:
                        stat = file_path.stat()
                        mtime = datetime.datetime.fromtimestamp(stat.st_mtime)
                        ctime = datetime.datetime.fromtimestamp(stat.st_ctime)
    
                        temporal_data['file_timestamps'].append({
                            'path': str(file_path.relative_to(self.project_path)),
                            'modified': mtime,
                            'created': ctime,
                            'size': stat.st_size
                        })
    
                        # Monthly aggregation
                        month_key = mtime.strftime('%Y-%m')
                        temporal_data['monthly_activity'][month_key] += 1
    
                    except (OSError, PermissionError):
                        pass
    
            # Detect work sessions (files modified within 4 hours = same session)
            if temporal_data['file_timestamps']:
                sorted_files = sorted(temporal_data['file_timestamps'], key=lambda x: x['modified'])
    
                current_session = {'start': None, 'end': None, 'files': [], 'file_count': 0}
                sessions = []
    
                for file_data in sorted_files:
                    if not current_session['start']:
                        current_session['start'] = file_data['modified']
                        current_session['end'] = file_data['modified']
                        current_session['files'].append(file_data['path'])
                        current_session['file_count'] = 1
                    else:
                        time_diff = (file_data['modified'] - current_session['end']).total_seconds() / 3600
    
                        if time_diff <= 4:  # Same session (within 4 hours)
                            current_session['end'] = file_data['modified']
                            current_session['files'].append(file_data['path'])
                            current_session['file_count'] += 1
                        else:
                            # Save current session and start new one
                            if current_session['file_count'] >= 3:  # Only meaningful sessions
                                duration_minutes = (current_session['end'] - current_session['start']).total_seconds() / 60
                                sessions.append({
                                    'start': current_session['start'].strftime('%Y-%m-%d %H:%M'),
                                    'end': current_session['end'].strftime('%Y-%m-%d %H:%M'),
                                    'duration_minutes': int(duration_minutes),
                                    'file_count': current_session['file_count'],
                                    'sample_files': current_session['files'][:5]
                                })
    
                            current_session = {
                                'start': file_data['modified'],
                                'end': file_data['modified'],
                                'files': [file_data['path']],
                                'file_count': 1
                            }
    
                # Don't forget last session
                if current_session['file_count'] >= 3:
                    duration_minutes = (current_session['end'] - current_session['start']).total_seconds() / 60
                    sessions.append({
                        'start': current_session['start'].strftime('%Y-%m-%d %H:%M'),
                        'end': current_session['end'].strftime('%Y-%m-%d %H:%M'),
                        'duration_minutes': int(duration_minutes),
                        'file_count': current_session['file_count'],
                        'sample_files': current_session['files'][:5]
                    })
    
                temporal_data['work_sessions'] = sorted(sessions, key=lambda x: x['start'], reverse=True)[:20]
    
            # Calculate project age
            if temporal_data['file_timestamps']:
                oldest = min(temporal_data['file_timestamps'], key=lambda x: x['created'])
                newest = max(temporal_data['file_timestamps'], key=lambda x: x['modified'])
    
                temporal_data['project_age_days'] = (newest['modified'] - oldest['created']).days
                temporal_data['oldest_file'] = oldest['path']
                temporal_data['newest_file'] = newest['path']
    
            self.temporal_analysis = temporal_data
            self.work_sessions = temporal_data['work_sessions']
            self.monthly_activity = dict(temporal_data['monthly_activity'])
    
            return temporal_data
    
        except Exception as e:
            logger.error(f"Temporal analysis failed: {e}")
            return temporal_data
    
    
    # ============================================================================
    # DUPLICATE DETECTION - Hash-based + Name similarity
    # ============================================================================
    

    def detect_duplicates(self) -> dict:
        """Detect duplicate files by content hash and similar names"""
        print("ðŸ” Detecting duplicates...")
    
        duplicate_data = {
            'exact_duplicates': [],  # Same content hash
            'similar_names': [],      # Similar filenames
            'potential_versions': [], # file_v1, file_v2 patterns
            'total_duplicate_size': 0
        }
    
        try:
            file_hashes = defaultdict(list)
            file_names = defaultdict(list)
    
            # Collect file hashes and names
            for root, dirs, files in os.walk(self.project_path):
                dirs[:] = [d for d in dirs if d not in ['node_modules', '__pycache__', '.git', 'dist', 'build']]
    
                for file in files:
                    file_path = Path(root) / file
    
                    try:
                        # Skip large files for hash calculation
                        if file_path.stat().st_size > 50_000_000:  # 50MB
                            continue
    
                        # Calculate hash for exact duplicates
                        with open(file_path, 'rb') as f:
                            file_hash = hashlib.md5(f.read()).hexdigest()
                            file_hashes[file_hash].append({
                                'path': str(file_path.relative_to(self.project_path)),
                                'size': file_path.stat().st_size,
                                'name': file
                            })
    
                        # Collect names for similarity analysis
                        file_names[file.lower()].append(str(file_path.relative_to(self.project_path)))
    
                    except (OSError, PermissionError, IOError):
                        pass
    
            # Find exact duplicates
            for file_hash, files in file_hashes.items():
                if len(files) > 1:
                    duplicate_data['exact_duplicates'].append({
                        'files': [f['path'] for f in files],
                        'count': len(files),
                        'size_each': files[0]['size'],
                        'total_wasted': files[0]['size'] * (len(files) - 1)
                    })
                    duplicate_data['total_duplicate_size'] += files[0]['size'] * (len(files) - 1)
    
            # Find similar names (potential duplicates)
            all_names = list(file_names.keys())
            for i, name1 in enumerate(all_names):
                for name2 in all_names[i+1:]:
                    similarity = SequenceMatcher(None, name1, name2).ratio()
                    if similarity > 0.85 and similarity < 1.0:  # Very similar but not identical
                        duplicate_data['similar_names'].append({
                            'name1': name1,
                            'name2': name2,
                            'similarity': f"{similarity:.1%}",
                            'paths1': file_names[name1],
                            'paths2': file_names[name2]
                        })
    
            # Detect version patterns (file_v1.txt, file_v2.txt, file_old.txt, etc)
            version_patterns = [r'_v\d+', r'_old', r'_new', r'_final', r'_backup', r'_copy', r'\(\d+\)']
            for name, paths in file_names.items():
                for pattern in version_patterns:
                    if re.search(pattern, name, re.IGNORECASE):
                        base_name = re.sub(pattern, '', name, flags=re.IGNORECASE)
                        if base_name in file_names:
                            duplicate_data['potential_versions'].append({
                                'base_name': base_name,
                                'versions': [name],
                                'paths': paths
                            })
    
            self.duplicate_analysis = duplicate_data
            return duplicate_data
    
        except Exception as e:
            logger.error(f"Duplicate detection failed: {e}")
            return duplicate_data
    
    
    # ============================================================================
    # NAMING PATTERN ANALYSIS - Conventions, Inconsistencies
    # ============================================================================
    

    def analyze_naming_patterns(self) -> dict:
        """Analyze naming conventions and detect inconsistencies"""
        print("ðŸ“ Analyzing naming patterns...")
    
        naming_data = {
            'conventions': {
                'camelCase': 0,
                'snake_case': 0,
                'kebab-case': 0,
                'PascalCase': 0,
                'SCREAMING_SNAKE_CASE': 0
            },
            'inconsistencies': [],
            'common_prefixes': Counter(),
            'common_suffixes': Counter(),
            'recommended_convention': None
        }
    
        try:
            all_names = []
    
            for root, dirs, files in os.walk(self.project_path):
                dirs[:] = [d for d in dirs if d not in ['node_modules', '__pycache__', '.git']]
    
                # Analyze directory names
                for dir_name in dirs:
                    all_names.append(dir_name)
    
                # Analyze filenames (without extension)
                for file in files:
                    name_without_ext = Path(file).stem
                    all_names.append(name_without_ext)
    
            # Detect conventions
            for name in all_names:
                if re.match(r'^[a-z]+([A-Z][a-z]+)+$', name):  # camelCase
                    naming_data['conventions']['camelCase'] += 1
                elif re.match(r'^[a-z]+(_[a-z]+)+$', name):  # snake_case
                    naming_data['conventions']['snake_case'] += 1
                elif re.match(r'^[a-z]+(-[a-z]+)+$', name):  # kebab-case
                    naming_data['conventions']['kebab-case'] += 1
                elif re.match(r'^[A-Z][a-z]+([A-Z][a-z]+)+$', name):  # PascalCase
                    naming_data['conventions']['PascalCase'] += 1
                elif re.match(r'^[A-Z]+(_[A-Z]+)+$', name):  # SCREAMING_SNAKE_CASE
                    naming_data['conventions']['SCREAMING_SNAKE_CASE'] += 1
    
            # Determine dominant convention
            if naming_data['conventions']:
                dominant = max(naming_data['conventions'].items(), key=lambda x: x[1])
                naming_data['recommended_convention'] = dominant[0]
    
            # Find common prefixes/suffixes
            for name in all_names:
                if '_' in name:
                    parts = name.split('_')
                    if len(parts) > 1:
                        naming_data['common_prefixes'][parts[0]] += 1
                        naming_data['common_suffixes'][parts[-1]] += 1
    
            naming_data['common_prefixes'] = dict(naming_data['common_prefixes'].most_common(10))
            naming_data['common_suffixes'] = dict(naming_data['common_suffixes'].most_common(10))
    
            self.naming_analysis = naming_data
            return naming_data
    
        except Exception as e:
            logger.error(f"Naming pattern analysis failed: {e}")
            return naming_data
    
    
    # ============================================================================
    # DIRECTORY PURPOSE CLASSIFICATION - Auto-detect purpose
    # ============================================================================
    

    def classify_directory_purposes(self) -> dict:
        """Classify directories by their purpose"""
        print("ðŸŽ¯ Classifying directory purposes...")
    
        purpose_data = {
            'purposes': {},
            'high_priority': [],
            'purpose_distribution': Counter()
        }
    
        purpose_keywords = {
            'testing': ['test', 'tests', '__tests__', 'spec', 'specs', 'e2e', 'integration'],
            'documentation': ['docs', 'documentation', 'guides', 'wiki', 'examples'],
            'source_code': ['src', 'lib', 'app', 'core', 'components', 'modules', 'services'],
            'configuration': ['config', 'conf', 'settings', '.config'],
            'data': ['data', 'datasets', 'fixtures', 'seeds'],
            'assets': ['assets', 'static', 'public', 'resources', 'images', 'media'],
            'scripts': ['scripts', 'bin', 'tools', 'utilities'],
            'build': ['build', 'dist', 'out', 'target', 'compiled'],
            'backup': ['backup', 'backups', 'archive', 'archives', 'old'],
            'temp': ['temp', 'tmp', 'cache', '.cache']
        }
    
        try:
            for root, dirs, files in os.walk(self.project_path):
                rel_path = Path(root).relative_to(self.project_path)
    
                # Classify directory
                dir_name = Path(root).name.lower()
                purpose = 'unknown'
    
                for purpose_type, keywords in purpose_keywords.items():
                    if any(keyword in dir_name for keyword in keywords):
                        purpose = purpose_type
                        break
    
                # Calculate importance
                file_count = len(files)
                dir_count = len(dirs)
    
                purpose_data['purposes'][str(rel_path)] = {
                    'purpose': purpose,
                    'files': file_count,
                    'subdirs': dir_count,
                    'priority': 'high' if purpose in ['source_code', 'testing', 'documentation'] else 'medium'
                }
    
                purpose_data['purpose_distribution'][purpose] += 1
    
                if purpose in ['source_code', 'testing'] and file_count > 10:
                    purpose_data['high_priority'].append({
                        'directory': str(rel_path),
                        'purpose': purpose,
                        'files': file_count
                    })
    
            self.directory_purposes = purpose_data
            return purpose_data
    
        except Exception as e:
            logger.error(f"Directory purpose classification failed: {e}")
            return purpose_data
    
    
    # ============================================================================
    # CONSOLIDATION OPPORTUNITIES - Smart merge suggestions
    # ============================================================================
    

    def find_consolidation_opportunities(self) -> list:
        """Find opportunities to consolidate/merge files and directories"""
        print("ðŸ”„ Finding consolidation opportunities...")
    
        opportunities = []
    
        try:
            # Use duplicate analysis
            if self.duplicate_analysis:
                # Exact duplicates can be consolidated
                for dup in self.duplicate_analysis.get('exact_duplicates', []):
                    if dup['count'] > 1:
                        opportunities.append({
                            'type': 'exact_duplicate',
                            'action': 'Delete duplicates, keep one',
                            'files': dup['files'],
                            'space_saved': f"{dup['total_wasted'] / 1024 / 1024:.2f} MB"
                        })
    
                # Version files can be consolidated
                for ver in self.duplicate_analysis.get('potential_versions', []):
                    opportunities.append({
                        'type': 'version_files',
                        'action': 'Use version control instead of manual versions',
                        'files': ver['paths'],
                        'suggestion': f"Consolidate versions of '{ver['base_name']}'"
                    })
    
            # Empty directories
            if self.empty_directories:
                opportunities.append({
                    'type': 'empty_directories',
                    'action': 'Remove empty directories',
                    'count': len(self.empty_directories),
                    'directories': self.empty_directories[:10]
                })
    
            # Multiple README files
            readme_files = []
            for root, dirs, files in os.walk(self.project_path):
                for file in files:
                    if file.lower().startswith('readme'):
                        readme_files.append(str(Path(root) / file))
    
            if len(readme_files) > 3:
                opportunities.append({
                    'type': 'scattered_documentation',
                    'action': 'Consolidate README files into central documentation',
                    'files': readme_files,
                    'count': len(readme_files)
                })
    
            self.consolidation_opportunities = opportunities
            return opportunities
    
        except Exception as e:
            logger.error(f"Consolidation opportunities detection failed: {e}")
            return opportunities
    
    
    # ============================================================================
    # EMPTY DIRECTORY DETECTION
    # ============================================================================
    

    def detect_empty_directories(self) -> list:
        """Find all empty directories"""
        empty_dirs = []
    
        try:
            for root, dirs, files in os.walk(self.project_path, topdown=False):
                for dir_name in dirs:
                    dir_path = Path(root) / dir_name
                    try:
                        if not any(dir_path.iterdir()):  # Empty
                            empty_dirs.append(str(dir_path.relative_to(self.project_path)))
                    except (OSError, PermissionError):
                        pass
    
            self.empty_directories = empty_dirs
            return empty_dirs
    
        except Exception as e:
            logger.error(f"Empty directory detection failed: {e}")
            return empty_dirs
    
    
    # ============================================================================
    # TECHNOLOGY STACK DETECTION
    # ============================================================================
    

    def detect_technology_stack(self) -> dict:
        """Detect technologies used in the project"""
        print("ðŸ”§ Detecting technology stack...")
    
        tech_stack = {
            'languages': Counter(),
            'frameworks': [],
            'tools': [],
            'package_managers': []
        }
    
        extension_to_language = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.ts': 'TypeScript',
            '.jsx': 'React (JSX)',
            '.tsx': 'React (TSX)',
            '.swift': 'Swift',
            '.java': 'Java',
            '.go': 'Go',
            '.rs': 'Rust',
            '.rb': 'Ruby',
            '.php': 'PHP',
            '.cs': 'C#',
            '.cpp': 'C++',
            '.c': 'C'
        }
    
        try:
            # Detect languages by file extensions
            for root, dirs, files in os.walk(self.project_path):
                dirs[:] = [d for d in dirs if d not in ['node_modules', '__pycache__']]
    
                for file in files:
                    ext = Path(file).suffix.lower()
                    if ext in extension_to_language:
                        tech_stack['languages'][extension_to_language[ext]] += 1
    
            # Detect frameworks/tools by config files
            framework_indicators = {
                'package.json': 'Node.js/npm',
                'requirements.txt': 'Python/pip',
                'Pipfile': 'Python/Pipenv',
                'Cargo.toml': 'Rust/Cargo',
                'go.mod': 'Go modules',
                'Gemfile': 'Ruby/Bundler',
                'composer.json': 'PHP/Composer',
                'pom.xml': 'Java/Maven',
                'build.gradle': 'Java/Gradle'
            }
    
            for root, dirs, files in os.walk(self.project_path):
                for file in files:
                    if file in framework_indicators:
                        tech_stack['package_managers'].append(framework_indicators[file])
    
            # Convert language counts to percentages
            total_files = sum(tech_stack['languages'].values())
            if total_files > 0:
                tech_stack['language_distribution'] = {
                    lang: f"{(count/total_files)*100:.1f}%"
                    for lang, count in tech_stack['languages'].most_common()
                }
    
            self.tech_stack = tech_stack
            return tech_stack
    
        except Exception as e:
            logger.error(f"Technology stack detection failed: {e}")
            return tech_stack
    
    
    # ============================================================================
    # GPT-5 INTEGRATION - LLM Analysis via Doppler
    # ============================================================================
    

    def analyze_with_gpt5(self) -> dict:
        """Send analysis data to GPT-5 for intelligent insights"""
        print("ðŸ¤– Analyzing with GPT-5...")
    
        llm_insights = {
            'curiosities': [],
            'smart_recommendations': [],
            'hidden_patterns': [],
            'project_purpose': '',
            'health_assessment': ''
        }
    
        try:
            # Get OpenAI API key from Doppler
            result = subprocess.run(
                ['doppler', 'secrets', 'get', 'OPENAI_API_KEY', '--plain'],
                capture_output=True,
                text=True
            )
    
            if result.returncode != 0:
                print("âš ï¸ Doppler not available, skipping LLM analysis")
                return llm_insights
    
            api_key = result.stdout.strip()
    
            # Prepare analysis summary for GPT-5
            analysis_summary = {
                'total_files': self.surface_scan.get('summary', {}).get('total_files', 0),
                'total_directories': self.surface_scan.get('summary', {}).get('total_directories', 0),
                'file_types': dict(self.surface_scan.get('summary', {}).get('file_types', {})),
                'tech_stack': self.tech_stack.get('language_distribution', {}),
                'duplicate_count': len(self.duplicate_analysis.get('exact_duplicates', [])),
                'empty_directories': len(self.empty_directories),
                'work_sessions': len(self.work_sessions),
                'monthly_activity': self.monthly_activity,
                'directory_purposes': self.directory_purposes.get('purpose_distribution', {})
            }
    
            # Call GPT-5 API
            import requests
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers={
                    'Authorization': f'Bearer {api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': 'gpt-4',  # Use gpt-4 or gpt-4-turbo (gpt-5 when available)
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'You are a expert project analyzer. Analyze the following project data and provide: 1) 3 interesting curiosities, 2) smart specific recommendations, 3) hidden patterns detected, 4) project purpose assessment, 5) health assessment. Be specific and insightful.'
                        },
                        {
                            'role': 'user',
                            'content': f'Analyze this project:\n\n{json.dumps(analysis_summary, indent=2)}'
                        }
                    ],
                    'temperature': 0.7,
                    'max_tokens': 1500
                },
                timeout=30
            )
    
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
    
                # Parse the response (simple parsing, could be improved)
                llm_insights['raw_response'] = content
                llm_insights['curiosities'] = ['Curiosity 1', 'Curiosity 2', 'Curiosity 3']  # Parse from content
                llm_insights['project_purpose'] = 'Detected from LLM analysis'
    
                print("âœ… GPT-5 analysis complete")
            else:
                print(f"âš ï¸ GPT-5 API error: {response.status_code}")
    
            self.llm_insights = llm_insights
            return llm_insights
    
        except Exception as e:
            logger.error(f"GPT-5 analysis failed: {e}")
            print(f"âš ï¸ LLM analysis skipped: {e}")
            return llm_insights

def main():
    """Main function"""
    import sys

    project_path = sys.argv[1] if len(sys.argv) > 1 else '.'
    analyzer = MrFixMyProjectPlease(project_path)

    try:
        results = analyzer.run_analysis()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Analysis interrupted by user")
    except Exception as e:
        print(f"âŒ Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()