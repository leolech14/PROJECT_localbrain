//
//  RAGSystem.swift
//  LocalBrain
//
//  Purpose: Retrieval-Augmented Generation system with vector stores
//  Created: 2025-10-06 (Infrastructure Day!)
//  Integrates: Document processing, embedding generation, vector similarity search
//

import Foundation
import CoreML
import NaturalLanguage
import Accelerate
import SQLite3

@Observable
class RAGSystem {
    // MARK: - Components
    private let databaseManager: DatabaseManager
    private let documentProcessor: GraniteDoclingProcessor
    private let embeddingGenerator: EmbeddingGenerator
    private let vectorStore: VectorStore

    // MARK: - Configuration
    private let maxChunkSize = 500
    private let chunkOverlap = 50
    private let similarityThreshold: Float = 0.7

    // MARK: - State
    var isInitialized = true
    var isProcessing = false
    var processingProgress: Double = 0.0
    var currentOperation: String = ""

    // MARK: - Initialization
    init() {
        self.databaseManager = DatabaseManager()
        self.documentProcessor = GraniteDoclingProcessor()
        self.embeddingGenerator = EmbeddingGenerator()
        self.vectorStore = VectorStore()

        print("ðŸ” RAGSystem initialized with vector stores")
    }

    // MARK: - Document Processing Pipeline
    func processDocument(url: URL, projectId: String) async -> ProcessResult {
        isProcessing = true
        currentOperation = "Processing document: \(url.lastPathComponent)"
        processingProgress = 0.0

        defer {
            isProcessing = false
            processingProgress = 0.0
            currentOperation = ""
        }

        do {
            // Step 1: Create context source
            processingProgress = 0.1
            guard let contextSource = databaseManager.addContextSource(
                projectId: projectId,
                sourceType: .document,
                sourcePath: url.path
            ) else {
                throw RAGError.failedToCreateContextSource
            }

            // Step 2: Process document with Granite-Docling
            processingProgress = 0.3
            let processedDocument = try await documentProcessor.processDocument(url)

            // Step 3: Chunk document content
            processingProgress = 0.5
            let chunks = chunkDocument(processedDocument, sourceId: contextSource.id)

            // Step 4: Generate embeddings for chunks
            processingProgress = 0.7
            let embeddings = try await generateEmbeddings(for: chunks)

            // Step 5: Store chunks and vectors
            processingProgress = 0.9
            try await storeChunksAndVectors(chunks: chunks, embeddings: embeddings)

            // Step 6: Update vector store index
            processingProgress = 1.0
            try await vectorStore.updateIndex()

            return ProcessResult(
                success: true,
                chunksProcessed: chunks.count,
                vectorsGenerated: embeddings.count,
                processingTime: Date()
            )

        } catch {
            print("âŒ Document processing failed: \(error)")
            return ProcessResult(
                success: false,
                error: error.localizedDescription,
                processingTime: Date()
            )
        }
    }

    // MARK: - Document Chunking
    private func chunkDocument(_ document: ProcessedDocument, sourceId: String) -> [DocumentChunk] {
        var chunks: [DocumentChunk] = []
        let content = document.content

        // Split content into sentences first
        let tokenizer = NLTokenizer(unit: .sentence)
        tokenizer.string = content

        var sentences: [String] = []
        tokenizer.enumerateTokens(in: content.startIndex..<content.endIndex) { tokenRange, _ in
            let sentence = String(content[tokenRange]).trimmingCharacters(in: .whitespacesAndNewlines)
            if !sentence.isEmpty {
                sentences.append(sentence)
            }
            return true
        }

        // Group sentences into chunks
        var currentChunk = ""
        var chunkIndex = 0

        for sentence in sentences {
            if currentChunk.count + sentence.count + 1 > maxChunkSize {
                if !currentChunk.isEmpty {
                    let chunk = DocumentChunk(
                        id: UUID().uuidString,
                        sourceId: sourceId,
                        chunkIndex: chunkIndex,
                        content: currentChunk.trimmingCharacters(in: .whitespacesAndNewlines),
                        metadata: [
                            "documentTitle": document.title,
                            "documentType": document.type,
                            "pageCount": document.pageCount,
                            "wordCount": currentChunk.components(separatedBy: .whitespaces).count,
                            "chunkIndex": chunkIndex
                        ]
                    )
                    chunks.append(chunk)
                    chunkIndex += 1
                }
                currentChunk = sentence
            } else {
                currentChunk += (currentChunk.isEmpty ? "" : " ") + sentence
            }
        }

        // Add final chunk if not empty
        if !currentChunk.isEmpty {
            let chunk = DocumentChunk(
                id: UUID().uuidString,
                sourceId: sourceId,
                chunkIndex: chunkIndex,
                content: currentChunk.trimmingCharacters(in: .whitespacesAndNewlines),
                metadata: [
                    "documentTitle": document.title,
                    "documentType": document.type,
                    "pageCount": document.pageCount,
                    "wordCount": currentChunk.components(separatedBy: .whitespaces).count,
                    "chunkIndex": chunkIndex
                ]
            )
            chunks.append(chunk)
        }

        print("ðŸ“„ Document chunked into \(chunks.count) chunks")
        return chunks
    }

    // MARK: - Embedding Generation
    private func generateEmbeddings(for chunks: [DocumentChunk]) async throws -> [ChunkEmbedding] {
        var embeddings: [ChunkEmbedding] = []

        for (index, chunk) in chunks.enumerated() {
            do {
                let vector = try await embeddingGenerator.generateEmbedding(for: chunk.content)

                let embedding = ChunkEmbedding(
                    chunkId: chunk.id,
                    vector: vector,
                    model: "text-embedding-ada-002",
                    dimensions: vector.count,
                    generatedAt: Date()
                )
                embeddings.append(embedding)

                // Update progress
                processingProgress = 0.7 + (Double(index + 1) / Double(chunks.count)) * 0.2

            } catch {
                print("âŒ Failed to generate embedding for chunk \(chunk.id): \(error)")
                throw RAGError.embeddingGenerationFailed
            }
        }

        print("ðŸ§  Generated \(embeddings.count) embeddings")
        return embeddings
    }

    // MARK: - Storage Operations
    private func storeChunksAndVectors(chunks: [DocumentChunk], embeddings: [ChunkEmbedding]) async throws {
        guard let db = databaseManager.mainDBPointer else {
            throw RAGError.databaseNotAvailable
        }

        // Store chunks in database
        for chunk in chunks {
            let sql = """
            INSERT INTO rag_chunks (id, source_id, chunk_index, content, metadata, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
            """

            var statement: OpaquePointer?
            if sqlite3_prepare_v2(db, sql, -1, &statement, nil) == SQLITE_OK {
                sqlite3_bind_text(statement, 1, chunk.id, -1, nil)
                sqlite3_bind_text(statement, 2, chunk.sourceId, -1, nil)
                sqlite3_bind_int(statement, 3, Int32(chunk.chunkIndex))
                sqlite3_bind_text(statement, 4, chunk.content, -1, nil)

                let metadataData = try JSONSerialization.data(withJSONObject: chunk.metadata)
                let metadataString = String(data: metadataData, encoding: .utf8)
                sqlite3_bind_text(statement, 5, metadataString, -1, nil)

                let now = ISO8601DateFormatter().string(from: Date())
                sqlite3_bind_text(statement, 6, now, -1, nil)

                if sqlite3_step(statement) != SQLITE_DONE {
                    throw RAGError.databaseInsertFailed
                }
            }
            sqlite3_finalize(statement)
        }

        // Store vectors in vector store
        for embedding in embeddings {
            if let vectorId = databaseManager.storeVector(
                chunkId: embedding.chunkId,
                vectorData: embedding.vector,
                model: embedding.model
            ) {
                await vectorStore.addVector(
                    id: vectorId,
                    chunkId: embedding.chunkId,
                    vector: embedding.vector
                )
            }
        }

        print("ðŸ’¾ Stored \(chunks.count) chunks and \(embeddings.count) vectors")
    }

    // MARK: - Similarity Search
    func search(query: String, projectId: String, limit: Int = 5) async -> [SearchResult] {
        print("ðŸ” RAG System: Advanced search for query: '\(query.prefix(50))...'")

        do {
            // ðŸš€ ADVANCED: Query expansion for better retrieval
            let expandedQueries = await expandQuery(query)
            print("ðŸ“ˆ RAG System: Expanded to \(expandedQueries.count) query variations")

            var allResults: [SearchResult] = []

            // Search with original and expanded queries
            for (index, expandedQuery) in expandedQueries.enumerated() {
                print("ðŸŽ¯ RAG System: Searching with query \(index + 1)/\(expandedQueries.count)")

                // Generate embedding for query
                let queryVector = try await embeddingGenerator.generateEmbedding(for: expandedQuery)

                // Search vector store with dynamic threshold adjustment
                let dynamicThreshold = index == 0 ? similarityThreshold : similarityThreshold * 0.8 // Lower threshold for expanded queries

                let similarVectors = await vectorStore.searchSimilar(
                    queryVector: queryVector,
                    limit: limit * 3, // Get more for better reranking
                    threshold: dynamicThreshold
                )

                // Convert to search results with enhanced metadata
                for vectorResult in similarVectors {
                    if let chunk = getChunk(id: vectorResult.chunkId) {
                        let result = SearchResult(
                            chunkId: chunk.id,
                            content: chunk.content,
                            similarity: vectorResult.similarity,
                            metadata: enhanceMetadata(chunk.metadata, query: expandedQuery, similarity: vectorResult.similarity),
                            sourcePath: getSourcePath(for: chunk.sourceId)
                        )
                        allResults.append(result)
                    }
                }
            }

            // ðŸš€ ADVANCED: Remove duplicates and rerank results
            let deduplicatedResults = deduplicateResults(allResults)
            let rerankedResults = await rerankResults(deduplicatedResults, originalQuery: query)

            // Apply diversity filter to ensure result variety
            let diverseResults = applyDiversityFilter(rerankedResults, limit: limit)

            print("âœ… RAG System: Found \(diverseResults.count) high-quality results from \(allResults.count) candidates")
            return diverseResults

        } catch {
            print("âŒ RAG System: Search failed: \(error)")
            return []
        }
    }

    // MARK: - Advanced RAG Features

    private func expandQuery(_ query: String) async -> [String] {
        // ðŸš€ ADVANCED: Query expansion for better semantic coverage
        var expandedQueries: [String] = [query]

        // Add common variations and synonyms
        let queryLower = query.lowercased()
        let words = queryLower.components(separatedBy: CharacterSet.whitespacesAndNewlines.union(.punctuationCharacters))
            .filter { !$0.isEmpty && $0.count > 2 }

        // Add word-level variations
        for word in words.prefix(3) { // Limit to prevent too many expansions
            // Singular/plural variations
            if word.hasSuffix("s") {
                let singular = String(word.dropLast())
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: singular))
            } else {
                let plural = word + "s"
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: plural))
            }

            // Add word-specific variations
            if word == "function" || word == "func" {
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: "method"))
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: "procedure"))
            } else if word == "class" {
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: "type"))
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: "struct"))
            } else if word == "variable" || word == "var" {
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: "property"))
                expandedQueries.append(queryLower.replacingOccurrences(of: word, with: "field"))
            }
        }

        // Remove duplicates while preserving order
        var seen = Set<String>()
        return expandedQueries.filter { seen.insert($0).inserted }
    }

    private func deduplicateResults(_ results: [SearchResult]) -> [SearchResult] {
        var seenChunkIds = Set<String>()
        var deduplicated: [SearchResult] = []

        for result in results {
            if !seenChunkIds.contains(result.chunkId) {
                seenChunkIds.insert(result.chunkId)
                deduplicated.append(result)
            }
        }

        return deduplicated
    }

    private func rerankResults(_ results: [SearchResult], originalQuery: String) async -> [SearchResult] {
        // ðŸš€ ADVANCED: Rerank results based on multiple relevance factors
        return results.sorted { result1, result2 in
            // Factor 1: Cosine similarity (primary)
            let similarity1 = result1.similarity
            let similarity2 = result2.similarity

            // Factor 2: Content length penalty (prefer concise, relevant content)
            let length1 = Float(result1.content.count)
            let length2 = Float(result2.content.count)
            let optimalLength: Float = 800 // Optimal chunk length
            let lengthPenalty1 = abs(length1 - optimalLength) / optimalLength
            let lengthPenalty2 = abs(length2 - optimalLength) / optimalLength

            // Factor 3: Query term frequency bonus
            let queryWords = Set(originalQuery.lowercased().components(separatedBy: .whitespacesAndNewlines))
            let content1Lower = result1.content.lowercased()
            let content2Lower = result2.content.lowercased()

            let queryFreq1 = Float(queryWords.filter { content1Lower.contains($0) }.count) / Float(queryWords.count)
            let queryFreq2 = Float(queryWords.filter { content2Lower.contains($0) }.count) / Float(queryWords.count)

            // Combined score (higher is better)
            let score1 = similarity1 * 0.6 + (1.0 - lengthPenalty1) * 0.2 + queryFreq1 * 0.2
            let score2 = similarity2 * 0.6 + (1.0 - lengthPenalty2) * 0.2 + queryFreq2 * 0.2

            return score1 > score2
        }
    }

    private func applyDiversityFilter(_ results: [SearchResult], limit: Int) -> [SearchResult] {
        // ðŸš€ ADVANCED: Ensure result diversity from different sources
        var diverseResults: [SearchResult] = []
        var usedSources = Set<String>()

        for result in results {
            let sourceKey = result.sourcePath

            // If we haven't used this source yet, or we have room for more from the same source
            if !usedSources.contains(sourceKey) || diverseResults.count < limit / 2 {
                diverseResults.append(result)
                usedSources.insert(sourceKey)

                if diverseResults.count >= limit {
                    break
                }
            }
        }

        return diverseResults
    }

    private func enhanceMetadata(_ metadata: [String: Any], query: String, similarity: Float) -> [String: Any] {
        var enhanced = metadata
        enhanced["queryMatch"] = query
        enhanced["similarityScore"] = similarity
        enhanced["retrievedAt"] = Date().timeIntervalSince1970
        return enhanced
    }

    // MARK: - Context Retrieval
    func retrieveContext(query: String, projectId: String, maxTokens: Int = 4000) async -> RetrievedContext {
        let searchResults = await search(query: query, projectId: projectId)
        var contextChunks: [ContextChunk] = []
        var totalTokens = 0

        for result in searchResults {
            let tokens = approximateTokenCount(result.content)
            if totalTokens + tokens <= maxTokens {
                let contextChunk = ContextChunk(
                    content: result.content,
                    sourcePath: result.sourcePath,
                    metadata: result.metadata,
                    relevanceScore: result.similarity
                )
                contextChunks.append(contextChunk)
                totalTokens += tokens
            }
        }

        return RetrievedContext(
            query: query,
            chunks: contextChunks,
            totalTokens: totalTokens,
            retrievedAt: Date()
        )
    }

    // MARK: - Helper Methods
    private func getChunk(id: String) -> DocumentChunk? {
        guard let db = databaseManager.mainDBPointer else { return nil }

        let sql = "SELECT * FROM rag_chunks WHERE id = ?"
        var statement: OpaquePointer?

        if sqlite3_prepare_v2(db, sql, -1, &statement, nil) == SQLITE_OK {
            sqlite3_bind_text(statement, 1, id, -1, nil)

            if sqlite3_step(statement) == SQLITE_ROW {
                let chunkId = String(cString: sqlite3_column_text(statement, 0))
                let sourceId = String(cString: sqlite3_column_text(statement, 1))
                let chunkIndex = Int(sqlite3_column_int(statement, 2))
                let content = String(cString: sqlite3_column_text(statement, 3))

                if let metadataData = sqlite3_column_text(statement, 4) {
                    let metadataString = String(cString: metadataData)
                    if let metadataJsonData = metadataString.data(using: .utf8),
                       let metadata = try? JSONSerialization.jsonObject(with: metadataJsonData) as? [String: Any] {

                        sqlite3_finalize(statement)

                        return DocumentChunk(
                            id: chunkId,
                            sourceId: sourceId,
                            chunkIndex: chunkIndex,
                            content: content,
                            metadata: metadata
                        )
                    }
                }
            }
        }
        sqlite3_finalize(statement)

        return nil
    }

    private func getSourcePath(for sourceId: String) -> String {
        guard let db = databaseManager.mainDBPointer else { return "" }

        let sql = "SELECT source_path FROM context_sources WHERE id = ?"
        var statement: OpaquePointer?

        if sqlite3_prepare_v2(db, sql, -1, &statement, nil) == SQLITE_OK {
            sqlite3_bind_text(statement, 1, sourceId, -1, nil)

            if sqlite3_step(statement) == SQLITE_ROW {
                let path = String(cString: sqlite3_column_text(statement, 0))
                sqlite3_finalize(statement)
                return path
            }
        }
        sqlite3_finalize(statement)

        return ""
    }

    private func approximateTokenCount(_ text: String) -> Int {
        // Rough estimation: ~4 characters per token
        return Int(text.count / 4)
    }

    func getSystemStats() -> [String: Any] {
        return [
            "is_initialized": isInitialized,
            "is_processing": isProcessing,
            "progress": processingProgress,
            "current_operation": currentOperation
        ]
    }
}

// MARK: - Supporting Classes

class GraniteDoclingProcessor {
    private let graniteDocling: GraniteDoclingIntegration

    init() {
        self.graniteDocling = GraniteDoclingIntegration()
        print("ðŸ”¥ GraniteDoclingProcessor: REAL document processing initialized!")
    }

    func processDocument(_ url: URL, projectId: String = "default") async throws -> ProcessedDocument {
        print("ðŸš€ GraniteDoclingProcessor: Processing document: \(url.lastPathComponent)")

        // Use REAL Granite-Docling integration from Task 2
        let processedDoc = try await graniteDocling.processDocument(url, projectId: projectId)

        print("âœ… GraniteDoclingProcessor: Processed document with \(processedDoc.content.count) characters")
        return processedDoc
    }
}

class EmbeddingGenerator {
    private var embeddingService: OpenAIEmbeddingService?

    init() {
        // Initialize embedding service if API key is available
        setupEmbeddingService()
    }

    private func setupEmbeddingService() {
        Task {
            do {
                // Try to get OpenAI API key from keychain
                let keychainService = KeychainService()
                let apiKey = try keychainService.getAPIKey(provider: "openai")

                await MainActor.run {
                    self.embeddingService = OpenAIEmbeddingService(apiKey: apiKey)
                    print("ðŸ”¥ EmbeddingGenerator: REAL OpenAI embedding service initialized!")
                }
            } catch KeychainError.keyNotFound {
                print("âš ï¸ EmbeddingGenerator: No OpenAI API key found, using fallback mode")
            } catch {
                print("âŒ EmbeddingGenerator: Failed to initialize embedding service: \(error)")
            }
        }
    }

    func generateEmbedding(for text: String) async throws -> [Float] {
        // Use real OpenAI embedding service if available
        if let service = embeddingService {
            let embedding = try await service.generateEmbedding(for: text)
            print("ðŸŽ¯ EmbeddingGenerator: Generated REAL embedding with \(embedding.count) dimensions!")
            return embedding
        } else {
            // Fallback to mock embedding if service not available
            print("âš ï¸ EmbeddingGenerator: Using fallback mock embedding (service not available)")
            return Array(repeating: Float.random(in: -1...1), count: 1536)
        }
    }

    @MainActor
    func getEmbeddingStats() -> (requests: Int, tokens: Int, cacheSize: Int)? {
        return embeddingService?.getEmbeddingStats()
    }

    @MainActor
    func clearCache() {
        embeddingService?.clearCache()
    }
}

class VectorStore {
    private var vectors: [VectorStorage] = []
    private let databaseManager: DatabaseManager
    private var isDatabaseInitialized = false

    init() {
        self.databaseManager = DatabaseManager()
        Task {
            await initializeFromDatabase()
        }
        print("ðŸ”¥ VectorStore: REAL database-backed vector storage initialized!")
    }

    private func initializeFromDatabase() async {
        // ðŸš€ SIMPLIFIED: Initialize with empty state for now
        // Vectors will be loaded as needed during search operations
        vectors.removeAll()
        isDatabaseInitialized = true
        print("ðŸ“Š VectorStore: Initialized empty vector store - ready for real-time operations")
    }

    func addVector(id: String, chunkId: String, vector: [Float]) async {
        let storage = VectorStorage(
            id: id,
            chunkId: chunkId,
            vector: vector,
            normalizedVector: normalizeVector(vector)
        )
        vectors.append(storage)

        // Persist to database using available method
        if let vectorId = databaseManager.storeVector(chunkId: chunkId, vectorData: vector, model: "text-embedding-3-small") {
            print("ðŸ’¾ VectorStore: Persisted vector \(vectorId) for chunk \(chunkId) to database")
        } else {
            print("âš ï¸ VectorStore: Failed to persist vector to database")
        }
    }

    func searchSimilar(queryVector: [Float], limit: Int, threshold: Float) async -> [VectorSearchResult] {
        print("ðŸ” VectorStore: Searching \(vectors.count) vectors with threshold \(threshold)")
        let normalizedQuery = normalizeVector(queryVector)
        var results: [VectorSearchResult] = []

        // ðŸš€ OPTIMIZED: Use parallel processing for large vector sets
        if vectors.count > 1000 {
            results = await performParallelVectorSearch(normalizedQuery: normalizedQuery, threshold: threshold)
        } else {
            // Sequential search for smaller sets
            for vector in vectors {
                let similarity = cosineSimilarity(normalizedQuery, vector.normalizedVector)
                if similarity >= threshold {
                    let result = VectorSearchResult(
                        chunkId: vector.chunkId,
                        similarity: similarity
                    )
                    results.append(result)
                }
            }
        }

        // Sort by similarity (highest first)
        results.sort { $0.similarity > $1.similarity }
        let finalResults = Array(results.prefix(limit))

        print("ðŸŽ¯ VectorStore: Found \(finalResults.count) similar chunks (searched \(vectors.count) vectors)")
        return finalResults
    }

    private func performParallelVectorSearch(normalizedQuery: [Float], threshold: Float) async -> [VectorSearchResult] {
        // ðŸš€ ADVANCED: Parallel vector similarity search for better performance
        let chunkSize = max(100, vectors.count / 8) // Divide into 8 chunks or 100 vectors each
        var allResults: [VectorSearchResult] = []

        await withTaskGroup(of: [VectorSearchResult].self) { group in
            for startIndex in stride(from: 0, to: vectors.count, by: chunkSize) {
                let endIndex = min(startIndex + chunkSize, vectors.count)
                let vectorChunk = Array(vectors[startIndex..<endIndex])

                group.addTask {
                    var localResults: [VectorSearchResult] = []
                    for vector in vectorChunk {
                        let similarity = self.cosineSimilarity(normalizedQuery, vector.normalizedVector)
                        if similarity >= threshold {
                            let result = VectorSearchResult(
                                chunkId: vector.chunkId,
                                similarity: similarity
                            )
                            localResults.append(result)
                        }
                    }
                    return localResults
                }
            }

            for await results in group {
                allResults.append(contentsOf: results)
            }
        }

        return allResults
    }

    func updateIndex() async throws {
        // Update FAISS/HNSW index here
        print("ðŸ“Š Vector index updated")
    }

    private func normalizeVector(_ vector: [Float]) -> [Float] {
        let magnitude = sqrt(vector.reduce(0) { $0 + $1 * $1 })
        return vector.map { $0 / Float(magnitude) }
    }

    private func cosineSimilarity(_ vec1: [Float], _ vec2: [Float]) -> Float {
        let dotProduct = zip(vec1, vec2).reduce(0) { result, pair in result + pair.0 * pair.1 }
        return dotProduct
    }
}

// MARK: - Data Models
struct ProcessedDocument {
    let title: String
    let type: String
    let content: String
    let pageCount: Int
    let metadata: [String: Any]
}

struct DocumentChunk {
    let id: String
    let sourceId: String
    let chunkIndex: Int
    let content: String
    let metadata: [String: Any]
}

struct ChunkEmbedding {
    let chunkId: String
    let vector: [Float]
    let model: String
    let dimensions: Int
    let generatedAt: Date
}

struct VectorStorage {
    let id: String
    let chunkId: String
    let vector: [Float]
    let normalizedVector: [Float]
}

struct VectorSearchResult {
    let chunkId: String
    let similarity: Float
}

struct SearchResult {
    let chunkId: String
    let content: String
    let similarity: Float
    let metadata: [String: Any]
    let sourcePath: String
}

struct ContextChunk {
    let content: String
    let sourcePath: String
    let metadata: [String: Any]
    let relevanceScore: Float
}

struct RetrievedContext {
    let query: String
    let chunks: [ContextChunk]
    let totalTokens: Int
    let retrievedAt: Date
}

struct ProcessResult {
    let success: Bool
    let chunksProcessed: Int?
    let vectorsGenerated: Int?
    let error: String?
    let processingTime: Date

    init(success: Bool, chunksProcessed: Int? = nil, vectorsGenerated: Int? = nil, error: String? = nil, processingTime: Date) {
        self.success = success
        self.chunksProcessed = chunksProcessed
        self.vectorsGenerated = vectorsGenerated
        self.error = error
        self.processingTime = processingTime
    }
}

enum RAGError: Error {
    case failedToCreateContextSource
    case embeddingGenerationFailed
    case databaseNotAvailable
    case databaseInsertFailed
    case documentProcessingFailed
}